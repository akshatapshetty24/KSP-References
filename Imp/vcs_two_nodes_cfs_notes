hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01    RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cfs-sg          node01    N          Y               OFFLINE
B  cfs-sg          node02     N          Y               OFFLINE
B  cvm             node01    Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  vrts_vea_cfs_int_cfsmount2 node01    Y          N               ONLINE
B  vrts_vea_cfs_int_cfsmount2 node02     Y          N               ONLINE

-- RESOURCES NOT PROBED
-- Group           Type                 Resource             System

E  cfs-sg          Mount                cfsmount1            node01
E  cfs-sg          Mount                cfsmount1            node02


hagrp -online cfs-sg -sys node01
VCS WARNING V-16-1-10159 Group cfs-sg is auto-disabled in cluster. This can happen if group is not probed on all alive nodes in group's SystemList or VCS engine is not running on all alive nodes in group's SystemList

 hagrp -display cfs-sg | grep SystemList
cfs-sg       SystemList            global           node01    1       node02        2




vxprint -g sharedg
TY NAME         ASSOC        KSTATE   LENGTH   PLOFFS   STATE    TUTIL0  PUTIL0
dg sharedg      sharedg      -        -        -        -        -       -

dm node-0_vmdk0_3 node-0_vmdk0_3 - 2147417808 - - -    -

v  share_vol    fsgen        ENABLED  2097152000 -      ACTIVE   -       -
pl share_vol-01 share_vol    ENABLED  2097152000 -      ACTIVE   -       -
sd node-0_vmdk0_3-01 share_vol-01 ENABLED 2097152000 0 - -      -



hares -delete cfsmount1
hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01    RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cfs-sg          node01    Y        N             OFFLINE
B  cfs-sg          node02     Y          N              OFFLINE
B  cvm             node01    Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  vrts_vea_cfs_int_cfsmount2 node01    Y          N               ONLINE
B  vrts_vea_cfs_int_cfsmount2 node02     Y          N               ONLINE




[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
[root@node01 ~]# ll /dev/vx/dsk/sharedg/share_vol /infa_shared
brw-------. 1 root root 199, 37000 Jun 24 04:01 /dev/vx/dsk/sharedg/share_vol

/infa_shared:
total 0
[root@node01 ~]#

Perfect ‚Äî let‚Äôs cleanly set up a new service group and mount resource to manage your /infa_shared Veritas clustered file system via VCS.
I‚Äôll give you step-by-step safe, production-grade commands for it.

üìå üìñ Prerequisites:
/dev/vx/dsk/sharedg/share_vol exists ‚úÖ

/infa_shared mount point exists ‚úÖ

cvm group is online ‚úÖ

Cluster status healthy ‚úÖ

üìå üìä New Service Group & Resource Setup
1Ô∏è‚É£ Create Service Group
bash
Copy
Edit
hagrp -add infa_sg
Add both nodes to the SystemList:

bash
Copy
Edit
hagrp -modify infa_sg SystemList node01 0 node02 1
2Ô∏è‚É£ Create Mount Resource
bash
Copy
Edit
hares -add infa_mount Mount
3Ô∏è‚É£ Configure Mount Resource Attributes
bash
Copy
Edit
hares -modify infa_mount MountPoint /infa_shared
hares -modify infa_mount BlockDevice /dev/vx/dsk/sharedg/share_vol
hares -modify infa_mount FSType vxfs
hares -modify infa_mount FsckOpt "-n"
hares -modify infa_mount MountOpt "cluster"
4Ô∏è‚É£ Link Resource to Service Group
bash
Copy
Edit
hares -group infa_sg -add infa_mount
5Ô∏è‚É£ Bring Service Group Online on one node
bash
Copy
Edit
hagrp -online infa_sg -sys node01
6Ô∏è‚É£ Check Status
bash
Copy
Edit
hastatus -sum
You should now see infa_sg online on node01 and infa_mount probed and mounted.

7Ô∏è‚É£ Verify Mount
bash
Copy
Edit
df -h /infa_shared
Confirm it‚Äôs mounted via Veritas CFS.

üìå üìí Optional: AutoStart Configuration (if desired)
If you want this service group to auto start:

bash
Copy
Edit
hagrp -modify infa_sg AutoStartList node01 node02
‚úÖ Final Check:
hastatus -sum ‚Äî verify ONLINE

df -h ‚Äî mount point accessible

mount ‚Äî should show mounted with vxfs and cluster option

hagrp -add infa_sg
hagrp -modify infa_sg SystemList node01 0 node02 1
hares -add infa_mount Mount infa_sg
hares -modify infa_mount MountPoint /infa_shared
hares -modify infa_mount BlockDevice /dev/vx/dsk/sharedg/share_vol
hares -modify infa_mount FSType vxfs
hares -modify infa_mount FsckOpt "-n" # not working
hares -modify infa_mount MountOpt rw,cluster,crw,delaylog,largefiles,ioerror=mdisable,noatime,nomtime
hares -modify infa_mount MountOpt "cluster"
hares -group infa_sg -add infa_mount
hagrp -online infa_sg -sys node01
hastatus -sum
df -h /infa_shared

gabconfig -a
cfsmountadm display


hares list
hares -display
hares -display infa_mount | grep -E "infa_shared|vxfs|sharedg"
hares -delete cfsvol1

hagrp -dep
hagrp -delete

haconf -makerw
haconf -dump -makero

hastop -all
hastop -all -force

/etc/VRTSvcs/conf/config/main.cf
hacf -verify /etc/VRTSvcs/conf/config

hagrp -link cvm infa_sg online local firm


hares -enable infa_mount -sys  node01
hares -enable infa_mount -sys  node02

hagrp -enable infa_sg -sys node01
hagrp -enable infa_sg -sys node02


hares -probe infa_mount -sys node01
hares -probe infa_mount -sys node02

hagrp -online infa_sg -sys node01
hagrp -online infa_sg -sys node02

[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     N          N               OFFLINE
B  infa_sg         node02     N          N               OFFLINE

-- RESOURCES NOT PROBED
-- Group           Type                 Resource             System

E  infa_sg         Mount                infa_mount           node01
E  infa_sg         Mount                infa_mount           node02



 hares -display infa_mount | grep -E "infa_shared|vxfs|sharedg"
infa_mount   ArgListValues         node01 MountPoint  1       /infa_shared    BlockDevice     1       /dev/vx/dsk/sharedg/share_vol FSType  1       vxfs    MountOpt        1       rw,cluster,crw,delaylog,largefiles,ioerror=mdisable,noatime,nomtime     FsckOpt       1       ""      SnapUmount      1       0       CkptUmount      1       1       BindUmount      1       1       OptCheck     10       CreateMntPt     1       0       MntPtPermission 1       ""      MntPtOwner      1       ""      MntPtGroup      1       ""   AccessPermissionChk      1       0       RecursiveMnt    1       0       VxFSMountLock   1       1       CacheRestoreAccess      1    0
infa_mount   ArgListValues         node02 MountPoint  1       /infa_shared    BlockDevice     1       /dev/vx/dsk/sharedg/share_vol FSType  1       vxfs    MountOpt        1       rw,cluster,crw,delaylog,largefiles,ioerror=mdisable,noatime,nomtime     FsckOpt       1       ""      SnapUmount      1       0       CkptUmount      1       1       BindUmount      1       1       OptCheck     10       CreateMntPt     1       0       MntPtPermission 1       ""      MntPtOwner      1       ""      MntPtGroup      1       ""   AccessPermissionChk      1       0       RecursiveMnt    1       0       VxFSMountLock   1       1       CacheRestoreAccess      1    0
infa_mount   BlockDevice           global           /dev/vx/dsk/sharedg/share_vol
infa_mount   FSType                global           vxfs
infa_mount   MountPoint            global           /infa_shared
[root@node01 ~]#




 hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     N          Y               OFFLINE
B  infa_sg         node02     N          Y               OFFLINE

-- RESOURCES NOT PROBED
-- Group           Type                 Resource             System

E  infa_sg         Mount                infa_mount           node01
E  infa_sg         Mount                infa_mount           node02




cat /etc/VRTSvcs/conf/config/main.cf
include "OracleASMTypes.cf"
include "types.cf"
include "CFSTypes.cf"
include "CRSResource.cf"
include "CSSD.cf"
include "CVMTypes.cf"
include "Db2udbTypes.cf"
include "MultiPrivNIC.cf"
include "OracleTypes.cf"
include "PrivNIC.cf"
include "SybaseTypes.cf"

cluster auxlab_poc (
        ProtocolNumber = 10000
        SecureClus = 1
        GuestGroups = { testgrp }
        UseFence = SCSI3
        HacliUserLevel = COMMANDROOT
        )

system node01 (
        )

system node02 (
        )

group cvm (
        SystemList = { node01 = 0, node02 = 1 }
        AutoFailOver = 0
        Parallel = 1
        AutoStartList = { node01, node02 }
        )

        CFSfsckd vxfsckd (
                ActivationMode @node01 = { sharedg = sw }
                ActivationMode @node02 = { sharedg = sw }
                )

        CVMCluster cvm_clus (
                CVMClustName = auxlab_poc
                CVMNodeId = { node01 = 0, node02 = 1 }
                CVMTransport = gab
                CVMTimeout = 200
                )

        CVMVxconfigd cvm_vxconfigd (
                Critical = 0
                CVMVxconfigdArgs = { syslog }
                )

        ProcessOnOnly vxattachd (
                Critical = 0
                PathName = "/bin/sh"
                Arguments = "- /usr/lib/vxvm/bin/vxattachd root"
                RestartLimit = 3
                )

        cvm_clus requires cvm_vxconfigd
        vxfsckd requires cvm_clus


        // resource dependency tree
        //
        //      group cvm
        //      {
        //      ProcessOnOnly vxattachd
        //      CFSfsckd vxfsckd
        //          {
        //          CVMCluster cvm_clus
        //              {
        //              CVMVxconfigd cvm_vxconfigd
        //              }
        //          }
        //      }


group infa_sg (
        SystemList = { node01 = 0, node02 = 1 }
        )

        Mount infa_mount (
                MountPoint = "/infa_shared"
                BlockDevice = "/dev/vx/dsk/sharedg/share_vol"
                FSType = vxfs
                MountOpt = "cluster"
                )



        // resource dependency tree
        //
        //      group infa_sg
        //      {
        //      Mount infa_mount
        //      }




[root@node01 VRTS]# lltstat -nvv
LLT node information:
    Node                 State    Link  Status  Address
   * 0 node01  OPEN
                                  ens161   UP      00:50:56:AC:F7:BA
                                  ens224   UP      00:50:56:AC:6C:77
     1 node02  OPEN
                                  ens161   UP      00:50:56:AC:15:C4
                                  ens224   UP      00:50:56:AC:26:92
     2                   CONNWAIT
                                  ens161   DOWN
                                  ens224   DOWN


[root@node02 ~]# lltstat -nvv
LLT node information:
    Node                 State    Link  Status  Address
     0 node01  OPEN
                                  ens161   UP      00:50:56:AC:F7:BA
                                  ens224   UP      00:50:56:AC:6C:77
   * 1 node02  OPEN
                                  ens161   UP      00:50:56:AC:15:C4
                                  ens224   UP      00:50:56:AC:26:92



[root@node02 ~]# vxfenadm -d

I/O Fencing Cluster Information:
================================

 Fencing Protocol Version: 201
 Fencing Mode: MAJORITY
 Cluster Members:

          0 (node01)
        * 1 (node02)

 RFSM State Information:
        node   0 in state  8 (running)
        node   1 in state  8 (running)

[root@node02 ~]#


hagrp -online infa_sg -sys node01
VCS WARNING V-16-1-10159 Group infa_sg is auto-disabled in cluster. This can happen if group is not probed on all alive nodes in group's SystemList or VCS engine is not running on all alive nodes in group's SystemList



[root@node01 ~]# hagrp -clear infa_sg -sys node01
VCS NOTICE V-16-1-10145 There are no non-persistent resources in the group that are faulted on the specified systems
[root@node01 ~]# hagrp -clear infa_sg -sys node02
VCS NOTICE V-16-1-10145 There are no non-persistent resources in the group that are faulted on the specified systems
[root@node01 ~]#


[root@node01 ~]# hares -state
#Resource     Attribute             System           Value
cvm_clus      State                 node01 ONLINE
cvm_clus      State                 node02 ONLINE
cvm_vxconfigd State                 node01 ONLINE
cvm_vxconfigd State                 node02 ONLINE
infa_mount    State                 node01 OFFLINE|STATE UNKNOWN
infa_mount    State                 node02 OFFLINE|STATE UNKNOWN
vxattachd     State                 node01 ONLINE
vxattachd     State                 node02 ONLINE
vxfsckd       State                 node01 ONLINE
vxfsckd       State                 node02 ONLINE
[root@node01 ~]#



infa_mount    State                 node01 OFFLINE|STATE UNKNOWN
infa_mount    State                 node02 OFFLINE|STATE UNKNOWN


hares -enable infa_mount -sys node01
hares -enable infa_mount -sys node02


hares -enable infa_mount -sys node01
VCS WARNING V-16-1-10603 Unknown option: -enable

[root@node01 ~]# hares -online infa_mount -sys node01
VCS WARNING V-16-1-10282 Group infa_sg for resource infa_mount is auto-disabled in cluster
[root@node01 ~]# hares -online infa_mount -sys node02
VCS WARNING V-16-1-10282 Group infa_sg for resource infa_mount is auto-disabled in cluster
[root@node01 ~]#


[root@node01 ~]# hagrp -state infa_sg
#Group       Attribute             System           Value
infa_sg      State                 node01 |OFFLINE|
infa_sg      State                 node02 |OFFLINE|
[root@node01 ~]#


[root@node01 ~]# hagrp -autoenable infa_sg -sys node01
[root@node01 ~]# hagrp -autoenable infa_sg -sys node02
[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     N          N               OFFLINE
B  infa_sg         node02     N          N               OFFLINE

-- RESOURCES NOT PROBED
-- Group           Type                 Resource             System

E  infa_sg         Mount                infa_mount           node01
E  infa_sg         Mount                infa_mount           node02
[root@node01 ~]# hares -online infa_mount -sys node01
VCS WARNING V-16-1-10283 Resource has not been probed on system node01
[root@node01 ~]#

[root@node01 ~]# hares -probe infa_mount -sys node01
[root@node01 ~]# hares -probe infa_mount -sys node02
[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     N          N               OFFLINE
B  infa_sg         node02     N          N               OFFLINE

-- RESOURCES NOT PROBED
-- Group           Type                 Resource             System

E  infa_sg         Mount                infa_mount           node01
E  infa_sg         Mount                infa_mount           node02
[root@node01 ~]#


[root@node01 ~]# hares -online infa_mount -sys node01
VCS WARNING V-16-1-10283 Resource has not been probed on system node01
[root@node01 ~]# hares -online infa_mount -sys node02
VCS WARNING V-16-1-10283 Resource has not been probed on system node02
[root@node01 ~]#


[root@node01 ~]# hares -state
#Resource     Attribute             System           Value
cvm_clus      State                 node01 ONLINE
cvm_clus      State                 node02 ONLINE
cvm_vxconfigd State                 node01 ONLINE
cvm_vxconfigd State                 node02 ONLINE
infa_mount    State                 node01 OFFLINE|STATE UNKNOWN
infa_mount    State                 node02 OFFLINE|STATE UNKNOWN
vxattachd     State                 node01 ONLINE
vxattachd     State                 node02 ONLINE
vxfsckd       State                 node01 ONLINE
vxfsckd       State                 node02 ONLINE
[root@node01 ~]# hares -online infa_mount -sys node01
VCS WARNING V-16-1-10283 Resource has not been probed on system node01
[root@node01 ~]# hares -online infa_mount -sys node02
VCS WARNING V-16-1-10283 Resource has not been probed on system node02
[root@node01 ~]# hagrp -state
#Group       Attribute             System           Value
cvm          State                 node01 |ONLINE|
cvm          State                 node02 |ONLINE|
infa_sg      State                 node01 |OFFLINE|
infa_sg      State                 node02 |OFFLINE|
[root@node01 ~]#



[root@node01 ~]# haconf -makerw
[root@node01 ~]# vxdctl -c mode
mode: enabled: cluster active - MASTER
master: node01
[root@node01 ~]# hagrp -autoenable infa_sg -sys node01
[root@node01 ~]# hagrp -autoenable infa_sg -sys node02
[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     N          N               OFFLINE
B  infa_sg         node02     N          N               OFFLINE

-- RESOURCES NOT PROBED
-- Group           Type                 Resource             System

E  infa_sg         Mount                infa_mount           node01
E  infa_sg         Mount                infa_mount           node02
[root@node01 ~]#

[root@node01 ~]# hagrp -online infa_sg -sys node01
VCS WARNING V-16-1-10162 Group infa_sg has not been fully probed on system node01
[root@node01 ~]# hagrp -online infa_sg -sys node02
VCS WARNING V-16-1-10162 Group infa_sg has not been fully probed on system node02
[root@node01 ~]# hares -probe infa_mount -sys node01
[root@node01 ~]# hares -probe infa_mount -sys node02
[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     N          N               OFFLINE
B  infa_sg         node02     N          N               OFFLINE

-- RESOURCES NOT PROBED
-- Group           Type                 Resource             System

E  infa_sg         Mount                infa_mount           node01
E  infa_sg         Mount                infa_mount           node02
[root@node01 ~]#



[root@node01 ~]# hagrp -state infa_sg
#Group       Attribute             System           Value
infa_sg      State                 node01 |OFFLINE|
infa_sg      State                 node02 |OFFLINE|
[root@node01 ~]# hares -state infa_mount
#Resource    Attribute             System           Value
infa_mount   State                 node01 OFFLINE|STATE UNKNOWN
infa_mount   State                 node02 OFFLINE|STATE UNKNOWN
[root@node01 ~]#



[root@node01 ~]# hares -display infa_mount | grep ArgListValues
infa_mount   ArgListValues         node01 MountPoint  1       /infa_shared    BlockDevice     1       /dev/vx/dsk/sharedg/share_vol FSType  1       vxfs    MountOpt        1       cluster FsckOpt 1       ""      SnapUmount      1       0       CkptUmount   11       BindUmount      1       1       OptCheck        1       0       CreateMntPt     1       0       MntPtPermission 1       ""   MntPtOwner       1       ""      MntPtGroup      1       ""      AccessPermissionChk     1       0       RecursiveMnt    1       0    VxFSMountLock    1       1       CacheRestoreAccess      1       0
infa_mount   ArgListValues         node02 MountPoint  1       /infa_shared    BlockDevice     1       /dev/vx/dsk/sharedg/share_vol FSType  1       vxfs    MountOpt        1       cluster FsckOpt 1       ""      SnapUmount      1       0       CkptUmount   11       BindUmount      1       1       OptCheck        1       0       CreateMntPt     1       0       MntPtPermission 1       ""   MntPtOwner       1       ""      MntPtGroup      1       ""      AccessPermissionChk     1       0       RecursiveMnt    1       0    VxFSMountLock    1       1       CacheRestoreAccess      1       0
[root@node01 ~]#



infa_mount   ArgListValues         node01 MountPoint  1       /infa_shared    BlockDevice     1       /dev/vx/dsk/sharedg/share_vol FSType  1       vxfs    MountOpt        1       cluster FsckOpt 1       ""      SnapUmount      1       0       CkptUmount   11       BindUmount      1       1       OptCheck        1       0       CreateMntPt     1       0       MntPtPermission 1       ""   MntPtOwner       1       ""      MntPtGroup      1       ""      AccessPermissionChk     1       0       RecursiveMnt    1       0    VxFSMountLock    1       1       CacheRestoreAccess      1       0



infa_mount   ArgListValues         node02 MountPoint  1       /infa_shared    BlockDevice     1       /dev/vx/dsk/sharedg/share_vol FSType  1       vxfs    MountOpt        1       cluster FsckOpt 1       ""      SnapUmount      1       0       CkptUmount   11       BindUmount      1       1       OptCheck        1       0       CreateMntPt     1       0       MntPtPermission 1       ""   MntPtOwner       1       ""      MntPtGroup      1       ""      AccessPermissionChk     1       0       RecursiveMnt    1       0    VxFSMountLock    1       1       CacheRestoreAccess      1       0


[root@node01 ~]# hares -modify infa_mount ArgListValues { \
> "MountPoint" "/infa_shared" \
> "BlockDevice" "/dev/vx/dsk/sharedg/share_vol" \
> "FSType" "vxfs" \
> "MountOpt" "cluster" \
> "FsckOpt" "" \
> "SnapUmount" "0" \
> "CkptUmount" "1" \
> "BindUmount" "1" \
> "OptCheck" "0" \
> "CreateMntPt" "0" \
> "MntPtPermission" "" \
> "MntPtOwner" "" \
> "MntPtGroup" "" \
> "AccessPermissionChk" "0" \
> "RecursiveMnt" "0" \
> "VxFSMountLock" "1" \
> "CacheRestoreAccess" "0" }
VCS WARNING V-16-1-10576 System-defined attributes may not be modified
[root@node01 ~]#

[root@node01 ~]# hares -modify infa_mount ArgListValues { "MountPoint" "/infa_shared" "BlockDevice" "/dev/vx/dsk/sharedg/share_vol" "FSType" "vxfs" "MountOpt" "cluster" "FsckOpt" "" "SnapUmount" "0" "CkptUmount" "1" "BindUmount" "1" "OptCheck" "0" "CreateMntPt" "0" "MntPtPermission" "" "MntPtOwner" "" "MntPtGroup" "" "AccessPermissionChk" "0" "RecursiveMnt" "0" "VxFSMountLock" "1" "CacheRestoreAccess" "0" }
VCS WARNING V-16-1-10576 System-defined attributes may not be modified
[root@node01 ~]# 



[root@node01 ~]# haconf -makerw
[root@node01 ~]# hares -delete infa_mount
[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     Y          N               OFFLINE
B  infa_sg         node02     Y          N               OFFLINE
[root@node01 ~]# hares -add infa_mount CFSMount infa_sg
VCS NOTICE V-16-1-10242 Resource added. Enabled attribute must be set before agent monitors
[root@node01 ~]# hares -modify infa_mount MountPoint "/infa_shared"
[root@node01 ~]# hares -modify infa_mount BlockDevice "/dev/vx/dsk/sharedg/share_vol"
[root@node01 ~]# hares -modify infa_mount MountOpt "cluster"
[root@node01 ~]# hares -display infa_mount | grep ArgListValues
infa_mount   ArgListValues         node01
infa_mount   ArgListValues         node02
[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     Y          N               OFFLINE
B  infa_sg         node02     Y          N               OFFLINE

-- RESOURCES NOT PROBED
-- Group           Type                 Resource             System

E  infa_sg         CFSMount             infa_mount           node01
E  infa_sg         CFSMount             infa_mount           node02
[root@node01 ~]#



[root@node01 ~]# hares -modify infa_mount Enabled 1
VCS WARNING V-16-1-11335 Configuration must be ReadWrite : Use haconf -makerw
[root@node01 ~]# haconf -makerw
[root@node01 ~]# hares -modify infa_mount Enabled 1
[root@node01 ~]# hares -display infa_mount | grep Enabled
infa_mount   Enabled               global           1
infa_mount   ResContainerInfo      global           Type                Name            Enabled
infa_mount   TriggersEnabled       global
[root@node01 ~]# hagrp -online infa_sg -sys node01
[root@node01 ~]# hagrp -online infa_sg -sys node02
[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     Y          N               ONLINE
B  infa_sg         node02     Y          N               ONLINE
[root@node01 ~]#



[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     Y          N               ONLINE
B  infa_sg         node02     Y          N               ONLINE
[root@node01 ~]# hacf -verify /etc/VRTSvcs/conf/config
[root@node01 ~]# haconf -dump -makero
[root@node01 ~]# df -Th
Filesystem                    Type      Size  Used Avail Use% Mounted on
devtmpfs                      devtmpfs   16G     0   16G   0% /dev
tmpfs                         tmpfs      16G     0   16G   0% /dev/shm
tmpfs                         tmpfs      16G  9.1M   16G   1% /run
tmpfs                         tmpfs      16G     0   16G   0% /sys/fs/cgroup
/dev/mapper/vg_01-LogVol00    ext4       45G   21G   23G  48% /
/dev/sda1                     ext4      474M  240M  206M  54% /boot
/dev/mapper/vg_01-LogVol01    ext4      9.6G  316K  9.1G   1% /home
/dev/mapper/vg_01-LogVol02    ext4       15G  674M   14G   5% /tmp
/dev/mapper/vg_01-LogVol03    ext4       20G  949M   18G   5% /var
/dev/mapper/vg_01-LogVol04    ext4       15G  1.3G   13G   9% /var/log
/dev/mapper/vg_01-lv_audit    ext4      997M   33M  896M   4% /var/log/audit
tmpfs                         tmpfs     3.2G     0  3.2G   0% /run/user/0
/dev/vx/dsk/sharedg/share_vol vxfs     1000G  797M  937G   1% /infa_shared
[root@node01 ~]# ll /infa_shared/
total 0
-rw-------. 1 root root  0 Jun 22 23:59 file123
-rw-------. 1 root root  0 Jun 24 17:13 file234
drwx------. 2 root root 96 Jun 22 23:57 lost+found
[root@node01 ~]#


[root@node02 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     Y          N               ONLINE
B  infa_sg         node02     Y          N               ONLINE
[root@node02 ~]# touch /infa_shared/
file123     lost+found/
[root@node02 ~]# touch /infa_shared/
file123     lost+found/
[root@node02 ~]# touch /infa_shared/file234
[root@node02 ~]# ll /infa_shared/
total 0
-rw-------. 1 root root  0 Jun 22 23:59 file123
-rw-------. 1 root root  0 Jun 24 17:13 file234
drwx------. 2 root root 96 Jun 22 23:57 lost+found
[root@node02 ~]#



[root@node01 ~]# gabconfig  -a
GAB Port Memberships
===============================================================
Port a gen   350002 membership 01
Port b gen   350006 membership 01
Port d gen   350001 membership 01
Port f gen   350011 membership 01
Port h gen   350005 membership 01
Port m gen   350008 membership 01
Port u gen   35000f membership 01
Port v gen   35000a membership 01
Port w gen   35000c membership 01
Port y gen   350009 membership 01
[root@node01 ~]# lltstat -nvv | head -10
LLT node information:
    Node                 State    Link  Status  Address
   * 0 node01  OPEN
                                  ens161   UP      00:50:56:AC:F7:BA
                                  ens224   UP      00:50:56:AC:6C:77
     1 node02  OPEN
                                  ens161   UP      00:50:56:AC:15:C4
                                  ens224   UP      00:50:56:AC:26:92
     2                   CONNWAIT
                                  ens161   DOWN
[root@node01 ~]# vxfenadm -d

I/O Fencing Cluster Information:
================================

 Fencing Protocol Version: 201
 Fencing Mode: MAJORITY
 Cluster Members:

        * 0 (node01)
          1 (node02)

 RFSM State Information:
        node   0 in state  8 (running)
        node   1 in state  8 (running)

[root@node01 ~]#
[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     Y          N               ONLINE
B  infa_sg         node02     Y          N               ONLINE
[root@node01 ~]# df -Th
Filesystem                    Type      Size  Used Avail Use% Mounted on
devtmpfs                      devtmpfs   16G     0   16G   0% /dev
tmpfs                         tmpfs      16G     0   16G   0% /dev/shm
tmpfs                         tmpfs      16G  9.1M   16G   1% /run
tmpfs                         tmpfs      16G     0   16G   0% /sys/fs/cgroup
/dev/mapper/vg_01-LogVol00    ext4       45G   21G   23G  48% /
/dev/sda1                     ext4      474M  240M  206M  54% /boot
/dev/mapper/vg_01-LogVol01    ext4      9.6G  316K  9.1G   1% /home
/dev/mapper/vg_01-LogVol02    ext4       15G  674M   14G   5% /tmp
/dev/mapper/vg_01-LogVol03    ext4       20G  949M   18G   5% /var
/dev/mapper/vg_01-LogVol04    ext4       15G  1.3G   13G   9% /var/log
/dev/mapper/vg_01-lv_audit    ext4      997M   34M  896M   4% /var/log/audit
tmpfs                         tmpfs     3.2G     0  3.2G   0% /run/user/0
/dev/vx/dsk/sharedg/share_vol vxfs     1000G  797M  937G   1% /infa_shared
[root@node01 ~]#



root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     Y          N               OFFLINE|STARTING
B  infa_sg         node02     Y          N               OFFLINE|STARTING

-- RESOURCES ONLINING
-- Group           Type            Resource             System               IState

F  infa_sg         CFSMount        infa_mount           node01     W_ONLINE
F  infa_sg         CFSMount        infa_mount           node02     W_ONLINE
[root@node01 ~]#



[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     Y          N               ONLINE
B  infa_sg         node02     Y          N               OFFLINE|FAULTED

-- RESOURCES FAILED
-- Group           Type                 Resource             System

D  infa_sg         CFSMount             infa_mount           node02
[root@node01 ~]#

[root@node01 ~]# df -Th
Filesystem                    Type      Size  Used Avail Use% Mounted on
devtmpfs                      devtmpfs   16G     0   16G   0% /dev
tmpfs                         tmpfs      16G     0   16G   0% /dev/shm
tmpfs                         tmpfs      16G  9.1M   16G   1% /run
tmpfs                         tmpfs      16G     0   16G   0% /sys/fs/cgroup
/dev/mapper/vg_01-LogVol00    ext4       45G   21G   23G  48% /
/dev/sda1                     ext4      474M  240M  206M  54% /boot
/dev/mapper/vg_01-LogVol01    ext4      9.6G  316K  9.1G   1% /home
/dev/mapper/vg_01-LogVol02    ext4       15G  674M   14G   5% /tmp
/dev/mapper/vg_01-LogVol03    ext4       20G  949M   18G   5% /var
/dev/mapper/vg_01-LogVol04    ext4       15G  1.3G   13G   9% /var/log
/dev/mapper/vg_01-lv_audit    ext4      997M   34M  895M   4% /var/log/audit
tmpfs                         tmpfs     3.2G     0  3.2G   0% /run/user/0
/dev/vx/dsk/sharedg/share_vol vxfs     1000G  797M  937G   1% /infa_shared
[root@node01 ~]#


[root@node01 ~]# hares -display infa_mount | grep ArgListValues
infa_mount   ArgListValues         node01 MountPoint  1       /infa_shared    BlockDevice     1       /dev/vx/dsk/sharedg/share_vol  MountOpt        1       cluster CloneSkip       1       no      Primary 1       node01        AMFMountType    1     vxfs
infa_mount   ArgListValues         node02 MountPoint  1       /infa_shared    BlockDevice     1       /dev/vx/dsk/sharedg/share_vol  MountOpt        1       cluster CloneSkip       1       no      Primary 1       node01        AMFMountType    1     vxfs
[root@node01 ~]# hagrp -clear infa_sg -sys node02
[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     Y          N               ONLINE
B  infa_sg         node02     Y          N               OFFLINE
[root@node01 ~]# hagrp -online infa_sg -sys node02
[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     Y          N               ONLINE
B  infa_sg         node02     Y          N               ONLINE
[root@node01 ~]#


[root@node01 ~]# hacf -verify /etc/VRTSvcs/conf/config
[root@node01 ~]# haconf -dump -makero
VCS WARNING V-16-1-10369 Cluster not writable.
[root@node01 ~]#



[root@node01 ~]# df -Th
Filesystem                    Type      Size  Used Avail Use% Mounted on
devtmpfs                      devtmpfs   16G     0   16G   0% /dev
tmpfs                         tmpfs      16G     0   16G   0% /dev/shm
tmpfs                         tmpfs      16G  9.1M   16G   1% /run
tmpfs                         tmpfs      16G     0   16G   0% /sys/fs/cgroup
/dev/mapper/vg_01-LogVol00    ext4       45G   21G   23G  48% /
/dev/sda1                     ext4      474M  240M  206M  54% /boot
/dev/mapper/vg_01-LogVol01    ext4      9.6G  316K  9.1G   1% /home
/dev/mapper/vg_01-LogVol02    ext4       15G  674M   14G   5% /tmp
/dev/mapper/vg_01-LogVol03    ext4       20G  949M   18G   5% /var
/dev/mapper/vg_01-LogVol04    ext4       15G  1.3G   13G   9% /var/log
/dev/mapper/vg_01-lv_audit    ext4      997M   36M  894M   4% /var/log/audit
tmpfs                         tmpfs     3.2G     0  3.2G   0% /run/user/0
/dev/vx/dsk/sharedg/share_vol vxfs     1000G  797M  937G   1% /infa_shared
[root@node01 ~]# mount  | grep /infa_shared
/dev/vx/dsk/sharedg/share_vol on /infa_shared type vxfs (rw,seclabel,mntlock=VCS,cluster,crw,delaylog,largefiles,ioerror=mdisable)
[root@node01 ~]#


[root@node01 ~]# vxdisk -o alldgs list
DEVICE          TYPE            DISK         GROUP        STATUS
node01_vmdk0_3 auto:cdsdisk    node01_vmdk0_3  sharedg      online shared
sda          auto:LVM        -            -            LVM
[root@node01 ~]# vxprint -ht
Disk group: sharedg

DG NAME         NCONFIG      NLOG     MINORS   GROUP-ID
ST NAME         STATE        DM_CNT   SPARE_CNT         APPVOL_CNT
DM NAME         DEVICE       TYPE     PRIVLEN  PUBLEN   STATE
RV NAME         RLINK_CNT    KSTATE   STATE    PRIMARY  DATAVOLS  SRL
RL NAME         RVG          KSTATE   STATE    REM_HOST REM_DG    REM_RLNK
CO NAME         CACHEVOL     KSTATE   STATE
VT NAME         RVG          KSTATE   STATE    NVOLUME
V  NAME         RVG/VSET/CO  KSTATE   STATE    LENGTH   READPOL   PREFPLEX UTYPE
PL NAME         VOLUME       KSTATE   STATE    LENGTH   LAYOUT    NCOL/WID MODE
SD NAME         PLEX         DISK     DISKOFFS LENGTH   [COL/]OFF DEVICE   MODE
SV NAME         PLEX         VOLNAME  NVOLLAYR LENGTH   [COL/]OFF AM/NM    MODE
SC NAME         PLEX         CACHE    DISKOFFS LENGTH   [COL/]OFF DEVICE   MODE
DC NAME         PARENTVOL    LOGVOL
SP NAME         SNAPVOL      DCO
EX NAME         ASSOC        VC                       PERMS    MODE     STATE
SR NAME         KSTATE

dg sharedg      default      default  37000    1750661268.10.node01.lab.uprising.t-mobile.com

dm node01_vmdk0_3 node01_vmdk0_3 auto 65536 2147417808 -

v  share_vol    -            ENABLED  ACTIVE   2097152000 SELECT  -        fsgen
pl share_vol-01 share_vol    ENABLED  ACTIVE   2097152000 CONCAT  -        RW
sd node01_vmdk0_3-01 share_vol-01 node01_vmdk0_3 0 2097152000 0 node01_vmdk0_3 ENA
[root@node01 ~]#



#!/bin/bash

# -----------------------------------------------------------------------------
# Veritas Cluster Server (VCS) Service Group and Resource Setup for CVM+CFS
# -----------------------------------------------------------------------------
# Author: You üòâ
# Usage: Run this on any cluster node where VCS CLI is configured
# -----------------------------------------------------------------------------

# 1Ô∏è‚É£ Check GAB membership (ensure CVM is running and cluster is healthy)
echo "Checking GAB port membership..."
gabconfig -a

# 2Ô∏è‚É£ Verify CVM service group state (should be ONLINE on both nodes)
echo "Checking existing service group states..."
hagrp -state

# 3Ô∏è‚É£ Create new service group: infa_sg
echo "Creating service group 'infa_sg'..."
hagrp -add infa_sg

# Set system list for the group
hagrp -modify infa_sg SystemList node01 0 node02 1

# Define autostart list
hagrp -modify infa_sg AutoStartList node01 node02

# Set group to be Parallel (for CFS mount groups)
hagrp -modify infa_sg AutoFailOver 0
hagrp -modify infa_sg Parallel 1

# 4Ô∏è‚É£ Add CFSMount resource: infa_mount
echo "Adding CFSMount resource 'infa_mount'..."
hares -add infa_mount CFSMount infa_sg

# Configure CFSMount resource attributes
hares -modify infa_mount MountPoint "/infa_shared"
hares -modify infa_mount BlockDevice "/dev/vx/dsk/sharedg/share_vol"
hares -modify infa_mount FSType vxfs
hares -modify infa_mount MountOpt "cluster"

# 5Ô∏è‚É£ Enable service group on both nodes
echo "Enabling service group on both nodes..."
hagrp -enable infa_sg -sys node01
hagrp -enable infa_sg -sys node02

# 6Ô∏è‚É£ Optional: Create dependency if needed (between cvm and infa_sg if both are Parallel)
echo "Creating group dependency (if applicable)..."
hagrp -link cvm infa_sg online local firm

# 7Ô∏è‚É£ Verify configuration integrity
echo "Verifying configuration..."
hacf -verify /etc/VRTSvcs/conf/config

# 8Ô∏è‚É£ Commit the config if no errors
echo "Committing configuration..."
hacf -commit /etc/VRTSvcs/conf/config

# 9Ô∏è‚É£ Bring infa_sg service group online on node01
echo "Bringing service group online on node01..."
hagrp -online infa_sg -sys node01

#  üîü Show final state summary
echo "Cluster status summary:"
hastatus -sum

echo "‚úÖ Configuration completed successfully."



[root@node01 ~]# cat vcs_health_check.sh
#!/bin/bash

# -----------------------------------------------------------------------------
# Veritas Cluster Server (VCS) Health Check Script
# -----------------------------------------------------------------------------
# Author: You üòâ
# Purpose: Check overall cluster, GAB, LLT, service groups, and resource states
# Usage: Run as root or VCS-enabled user on any cluster node
# -----------------------------------------------------------------------------

#CLUSTER_NAME=$(haconf -display | grep "ClusterName" | awk '{print $3}')
#echo "üì° Cluster Name: $CLUSTER_NAME"
#echo "===================="

# Fetch cluster name from main.cf
CLUSTER_NAME=$(grep -i '^cluster' /etc/VRTSvcs/conf/config/main.cf | awk '{print $2}' | tr -d '()')

echo "üì° Cluster Name: $CLUSTER_NAME"
echo "===================="


# 1Ô∏è‚É£ Check LLT links and port status
echo "üîç LLT Link Status:"
#lltstat -nvv | grep -E "Node|Link|Status"
lltstat -nvv | head -10
echo "--------------------"

# 2Ô∏è‚É£ Check GAB port membership
echo "üîç GAB Port Memberships:"
gabconfig -a

echo "--------------------"

# 3Ô∏è‚É£ Check Cluster Systems' Status
echo "üîç Cluster Nodes Status:"
hastatus -sum | grep "SYSTEM STATE" -A 5

echo "--------------------"

# 4Ô∏è‚É£ Check Service Groups State Summary
echo "üîç Service Groups Summary:"
hastatus -sum | grep "GROUP STATE" -A 10

echo "--------------------"

# 5Ô∏è‚É£ Check for any failed resources
echo "üîç Failed Resources (if any):"
hastatus -sum | grep "RESOURCES FAILED" -A 5

echo "--------------------"

# 6Ô∏è‚É£ Check specific service group status (example: cvm, infa_sg)
for grp in cvm infa_sg; do
  echo "üîç Service Group: $grp"
  hagrp -state $grp
  echo "--------------------"
done

# 7Ô∏è‚É£ Check if any groups are frozen
echo "üîç Frozen Groups:"
hagrp -list | grep -i Frozen

echo "--------------------"

# 8Ô∏è‚É£ Check for offline/faulted resources
echo "üîç Resources Offline/Faulted:"
hares -state | egrep "OFFLINE|FAULTED"

echo "===================="
echo "‚úÖ Cluster health check completed."

[root@node01 ~]#






Excellent ‚Äî let‚Äôs carefully unpack what you‚Äôve been doing, step by step, with context and understanding.
You‚Äôre managing a Veritas Cluster Server (VCS) setup for clustered filesystem (CFS) storage ‚Äî most likely on a pair of Linux servers, with CVM (Veritas Volume Manager Cluster) managing shared storage, CFS for cluster-wide file system mounts, and VCS clustering service groups and resources.

üìñ Setup Overview:
We‚Äôll break it down cleanly:

üìå Cluster Components
Component	Purpose
LLT	Low Latency Transport ‚Äî node-to-node heartbeat and messaging
GAB	Group Membership & Atomic Broadcast ‚Äî cluster coordination
CVM	Cluster Volume Manager ‚Äî shared disk group and volume management
CFS	Cluster File System ‚Äî shared file system mounting across nodes
VCS Service Groups	Logical groups managing resources as a unit
Mount/CFSMount Resource	Resource to manage mount points in VCS

üìå Your VCS Service Groups:
cvm ‚Äî a Parallel group (runs on all nodes)

infa_sg ‚Äî another Parallel group for your shared mount /infa_shared

üìñ Step-by-Step Breakdown:
‚úÖ 1Ô∏è‚É£ Base Cluster Setup:
LLT and GAB configuration done via /etc/llttab and /etc/gabtab

Verified using:

bash
Copy
Edit
gabconfig -a
lltstat -nvv
‚úÖ 2Ô∏è‚É£ CVM Group Configuration (main.cf):
group cvm configured as Parallel

Contains:

CVMVxconfigd ‚Äî manages VxVM

CVMCluster ‚Äî cluster-wide config

vxattachd ‚Äî handles disk attachment

CFSfsckd ‚Äî fsck daemon for vxfs volumes

Dependencies setup:

plaintext
Copy
Edit
CVMVxconfigd ‚Üí CVMCluster ‚Üí CFSfsckd
‚úÖ 3Ô∏è‚É£ Create and Configure Disk Group and Volume:
Created a shared disk group (say sharedg) and volume (share_vol) via VxVM commands.

Check:

bash
Copy
Edit
vxdisk -o alldgs list
vxprint -ht
‚úÖ 4Ô∏è‚É£ CFS Mount Point Creation:
Create the mount directory /infa_shared

Confirm device /dev/vx/dsk/sharedg/share_vol exists

‚úÖ 5Ô∏è‚É£ Create Service Group infa_sg
bash
Copy
Edit
hagrp -add infa_sg
hagrp -modify infa_sg SystemList node01 0 node02 1
hagrp -modify infa_sg AutoStartList node01 node02
‚úÖ 6Ô∏è‚É£ Add Mount Resource to infa_sg
bash
Copy
Edit
hares -add infa_mount CFSMount infa_sg
hares -modify infa_mount MountPoint "/infa_shared"
hares -modify infa_mount BlockDevice "/dev/vx/dsk/sharedg/share_vol"
hares -modify infa_mount FSType vxfs
hares -modify infa_mount MountOpt "cluster"
‚úÖ 7Ô∏è‚É£ Verify ArgListValues:
bash
Copy
Edit
hares -display infa_mount | grep ArgListValues
To confirm all resource attribute values are properly configured.

‚úÖ 8Ô∏è‚É£ Enable and Probe Group
bash
Copy
Edit
hagrp -enable infa_sg -sys node01
hagrp -enable infa_sg -sys node02
‚úÖ 9Ô∏è‚É£ Check Cluster Status
bash
Copy
Edit
hastatus -sum
Ensuring group is probed (Y) and no faults.

‚úÖ üîç Dependency Issue You Encountered:
You tried to create a dependency between cvm (Parallel group) and infa_sg (Parallel group initially set as Failover, hence problem).
Parallel group-to-Parallel group dependencies are generally discouraged or unsupported in VCS.

If both are Parallel, it's better to design via probe order and AutoStartList.

‚úÖ üîÑ Restart Cluster Services (if needed)
bash
Copy
Edit
hastop -all
hastart
Or individually:

bash
Copy
Edit
hastop -local -force
hastart
‚úÖ üîç Final Health Checks
Custom script or:

bash
Copy
Edit
gabconfig -a
lltstat -nvv
hastatus -sum
hares -state
üìñ Explanation of Key Attributes:
Attribute	Meaning
AutoFailOver = 0	Group won‚Äôt automatically fail over on fault
Parallel = 1	Group runs concurrently on multiple nodes
MountOpt = cluster	Allows CFS cluster mount
AutoStartList	Nodes where the group should auto-start

‚úÖ Summary
You essentially configured a two-node cluster with:

Shared CVM cluster

CFS shared storage mount

Proper VCS service group and resource configuration

Verified and managed service group state transitions

Resolved dependency misconfiguration issue (no Failover-Parallel mix dependency)





  ****************************************************************************************



  pl-p1-aux-05
/dev/vx/dsk/infashared_dg/infashared_vol vxfs       12T   11T  1.7T  86% /infa_shared

pl-p1-aux-06

https://sort.veritas.com/
https://sort.veritas.com/documents/doc_details/vie/7.4.2/Linux/Documentation/
https://sort.veritas.com/DocPortal/pdf/79638609-141543640-1
https://sort.veritas.com/DocPortal/pdf/79798461-141543598-1
https://sort.veritas.com/DocPortal/pdf/79664151-141543666-1
https://sort.veritas.com/checklist/install#report
https://sort.veritas.com/DocPortal/pdf/109508799-141543583-1
https://sort.veritas.com/DocPortal/pdf/81080974-141543550-1


[root@node02 ~]# export PATH=$PATH:/opt/VRTS/bin
[root@node02 ~]# hastatus -sum


https://node01-traf.lab.uprising.t-mobile.com:5634/vcs/admin/application_health.html
https://node01-traf.lab.uprising.t-mobile.com:5634/


CRQ000000037626  || Q2 2024 OS Patching & Veritas upgrade on Prod AWS US-EAST BSCS Veritas Servers[7] (Passive site)
CRQ000000037820 || Resume VVR replication between Prod AWS BSCS US-WEST and BSCS US-EAST Servers
CRQ000000038551 - Q4 2024 OS Patching & Veritas upgrade on Prod AWS US-EAST BSCS Veritas Servers[7] (Passive site)
White board & SIS Review - VVR Live | 8th NOD | CRQ000000036817


https://sort.veritas.com/documents/doc_details/vie/7.4.2/Linux/Documentation/
https://sort.veritas.com/checklist/install#report


https://sort.veritas.com/DocPortal/pdf/79664151-141543666-1
https://sort.veritas.com/DocPortal/pdf/79798461-141543598-1
https://sort.veritas.com/DocPortal/pdf/79638609-141543640-1



Hello, With RHEL8 and Infoscale 7.4.2 setup, here is a guide to assist you with installing and configuring the product on VMware.   1. For IO fencing and storage configuration on VMs. 

https://sort.veritas.com/DocPortal/pdf/81080974-141543550-1


2. This section will explain about installing and configuring Infoscale. You can start from page 19, skip the CP server section, and got for configuration section.
https://sort.veritas.com/DocPortal/pdf/79757062-141543634-1 

Hello, In addition to previous information, here is a list of kernel qualified for this product and patches that can be installed.
https://sort.veritas.com/kernel Installation guide:https://sort.veritas.com/DocPortal/pdf/109508799-141543583-1 

Patch: 
1. Base bundle patch 
   https://www.veritas.com/content/support/en_US/downloads/detail.REL319342#item2 
           https://www.veritas.com/support/en_US/downloads/detail.REL319342#item2

[root@node01 rhel8_x86_64]# pwd
/root/vcs_files/dvd1-redhatlinux/rhel8_x86_64
[root@node01 rhel8_x86_64]# ./installer -precheck

node01 node02


2. InfoScale 7.4.2 Update 8 Cumulative Patch on RHEL8 Platform (7.4.2.5600)
   https://www.veritas.com/content/support/en_US/downloads/update.UPD612114  

/root/vcs_files/installVRTSinfoscale-Patch
./installVRTSinfoscale742P5600 -require /root/vcs_files/cpipatch/patches/CPI_7.4.2_P40.pl node01 node02


3. cpi-Patch-7.4.2.4000 
   https://www.veritas.com/content/support/en_US/downloads/update.UPD156439


Server	NMNet	COSNet
node01	10.253.10.131	10.5.101.53
node02	10.253.10.132	10.5.101.54
pl-l1-aux-nfs-01	10.253.10.133	10.5.101.55
pl-l1-aux-nfs-02	10.253.10.134	10.5.101.56


192.168.47.0/24


[root@node02 ~]# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens161: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:50:56:ac:15:c4 brd ff:ff:ff:ff:ff:ff
    altname enp4s0
    inet 192.168.47.6/24 brd 192.168.47.255 scope global noprefixroute ens161
       valid_lft forever preferred_lft forever
    inet6 fe80::bf30:6607:a7d5:6daf/64 scope link noprefixroute
       valid_lft forever preferred_lft forever
3: ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:50:56:ac:ec:99 brd ff:ff:ff:ff:ff:ff
    altname enp11s0
    inet 10.253.10.132/24 brd 10.253.10.255 scope global noprefixroute ens192
       valid_lft forever preferred_lft forever
    inet6 fe80::d69a:fb79:5549:754d/64 scope link noprefixroute
       valid_lft forever preferred_lft forever
4: ens224: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:50:56:ac:26:92 brd ff:ff:ff:ff:ff:ff
    altname enp19s0
    inet 10.5.101.54/24 brd 10.5.101.255 scope global noprefixroute ens224
       valid_lft forever preferred_lft forever
    inet6 fe80::bae:f524:985e:2b1d/64 scope link noprefixroute
       valid_lft forever preferred_lft forever
[root@node02 ~]#


                                                   node01 node02

     1)  Configure the heartbeat links using LLT over Ethernet
     2)  Configure the heartbeat links using LLT over UDP
     3)  Configure the heartbeat links using LLT over TCP
     4)  Configure the heartbeat links using LLT over RDMA
     5)  Automatically detect configuration for LLT over Ethernet
     b)  Back to previous menu

How would you like to configure heartbeat links? [1-5,b,q,?] (5) 1

    Discovering NICs on node01 ......................................................... Discovered ens161 ens192 ens224

Enter the NIC for the first private heartbeat link on node01: [b,q,?] (ens161)

Would you like to configure a second private heartbeat link? [y,n,q,b,?] (n)

Enter the NIC for the low-priority heartbeat link on node01: [b,q,?] (ens224)




[root@npe-esa-pfa-w2a-rp1 repos]# ll /repos/infoscale_7.4.2_base
total 1048512
-rw-r-----. 1 root root 973012992 Jun 20 02:45 Veritas_InfoScale_7.4.2_RHEL.tar.gz
[root@npe-esa-pfa-w2a-rp1 repos]#


 Veritas InfoScale Enterprise 7.4.2 Install Program
                                                   node01 node02
Veritas:

/opt/VRTS/install/showversion

/opt/VRTSvcs/bin/hastatus -sum

‚Ä¢	Run utilities explorer & VRTSexplorer data from all nodes.
      #/opt/VRTSspt/VRTSexplorer/VRTSexplorer
Or  #/opt/VRTSspt/DataCollector/sort_linux_x64.sh
# hastatus -sum
#vxdisk -o alldgs -e list
#vxprint -htq
#vxdg list
vxdisk -o alldgs list
# hares -state
# gabconfig -a
# vxtask list
# vxprint -Pl
# vxprint -htq
vxprint -Pl
vxdctl -c mode


üìå Health Check Summary Table
Check	Command
LLT links	lltstat -nvv | head -10
GAB ports	gabconfig -a
Fencing status	vxfenadm -d
Cluster status (HAD)	hastatus -sum
Disk group/volumes	vxdg list / vxdisk -o alldgs list
CVM cluster status	vxclustadm -m vcs nidmap
CFS mount status	`mount
Service group switch test	hagrp -switch
Log review	tail -f /var/VRTSvcs/log/engine_A.log

hagrp -add infa_sg
hagrp -modify infa_sg SystemList node01 0 node02 1
hares -add infa_mount Mount infa_sg
hares -modify infa_mount MountPoint /infa_shared
hares -modify infa_mount BlockDevice /dev/vx/dsk/sharedg/share_vol
hares -modify infa_mount FSType vxfs
hares -modify infa_mount FsckOpt "-n" # not working
hares -modify infa_mount MountOpt rw,cluster,crw,delaylog,largefiles,ioerror=mdisable,noatime,nomtime
hares -modify infa_mount MountOpt "cluster"
hares -group infa_sg -add infa_mount
hagrp -online infa_sg -sys node01
hastatus -sum
df -h /infa_shared

gabconfig -a
systemctl status gab
systemctl start gab
systemctl enable gab
gabconfig -c -n 2
GAB driver already configured. Honored other options.

Confirm LLT, GAB, and HAD services started automatically:
systemctl restart llt
lltstat -nvv
ps -ef | grep -E 'had|hashadow'

---------------------------------------------------------------------

hares -delete infa_mount
hares -add infa_mount CFSMount infa_sg
hares -modify infa_mount MountPoint "/infa_shared"
hares -modify infa_mount BlockDevice "/dev/vx/dsk/sharedg/share_vol"
hares -modify infa_mount MountOpt "cluster"


‚úÖ Clean Summary:
hagrp -enable infa_sg -sys node01

hares -modify infa_mount Enabled 1

hagrp -online infa_sg -sys node01

hastatus -sum ‚Üí verify

---------------------------------------------------------------------

hagrp -enable infa_sg -sys node01
hagrp -enable infa_sg -sys node02

hagrp -probe infa_sg -sys node01
hagrp -probe infa_sg -sys node02


Why had.service Doesn't Exist
VCS HAD (High Availability Daemon) is usually started via the hastart command, after LLT and GAB are up.

Only llt and gab have proper systemd unit files (llt.service, gab.service)

had is controlled by Veritas itself via cluster control scripts.




cfsmountadm display


hares list
hares -display
hares -display infa_mount | grep -E "infa_shared|vxfs|sharedg"
hares -delete cfsvol1

hagrp -dep
hagrp -delete

haconf -makerw
hacf -verify /etc/VRTSvcs/conf/config
haconf -dump -makero

hastop -all
hastop -all -force

cat /etc/VRTSvcs/conf/config/main.cf | wc -l

To ensure no config issues.
hacf -verify /etc/VRTSvcs/conf/config


hagrp -link cvm infa_sg online local firm


hares -enable infa_mount -sys  node01
hares -enable infa_mount -sys  node02

hagrp -enable infa_sg -sys node01
hagrp -enable infa_sg -sys node02


hares -probe infa_mount -sys node01
hares -probe infa_mount -sys node02

hagrp -online infa_sg -sys node01
hagrp -online infa_sg -sys node02



üìå Veritas Cluster New Setup ‚Äî Health Check Checklist ‚úÖ
‚úÖ 1Ô∏è‚É£ Check LLT (Low Latency Transport) Links
On both nodes:

bash
Copy
Edit
lltstat -nvv
‚úîÔ∏è Confirm:

Both nodes listed

Private links up and healthy

‚úÖ 2Ô∏è‚É£ Check GAB (Group Membership) Status
bash
Copy
Edit
gabconfig -a
‚úîÔ∏è Expected ports:

a (LLT)

b (GAB)

f (Fencing ‚Äî if configured)

h (HAD/Cluster Server)

v (CVM)

m (CFS)

Others as per your setup

‚úîÔ∏è Both nodes should be in membership lists for relevant ports.

‚úÖ 3Ô∏è‚É£ Check Fencing Status (if configured)
bash
Copy
Edit
vxfenadm -d
‚úîÔ∏è Confirm:

Fencing Mode (SCSI3 / Majority / Disabled)

Cluster members listed

All nodes in state 8 (running)

Check fencing coordination points:

bash
Copy
Edit
vxfenadm -g
‚úîÔ∏è Should list disks or CP server info

‚úÖ 4Ô∏è‚É£ Check Cluster Server (HAD) Status
bash
Copy
Edit
hastatus -sum
‚úîÔ∏è Look for:

Service groups online on correct nodes

No FAULTED resources

‚úÖ 5Ô∏è‚É£ Check Disk Group and Volume Status
On one node:

bash
Copy
Edit
vxdg list
vxdisk -o alldgs list
‚úîÔ∏è Disk groups should be enabled
‚úîÔ∏è Volumes should be STARTED

‚úÖ 6Ô∏è‚É£ Check Cluster Volume Manager (CVM)
bash
Copy
Edit
gabconfig -a | grep v
‚úîÔ∏è Port v should show both nodes in membership
Or:

bash
Copy
Edit
vxclustadm -m vcs nidmap
‚úîÔ∏è Both nodes should be listed as enabled

‚úÖ 7Ô∏è‚É£ Check Cluster File System (CFS)
Check mounts:

bash
Copy
Edit
df -h
mount | grep vxfs
‚úîÔ∏è CFS mountpoints should be mounted with -o cluster
Validate file system health:

bash
Copy
Edit
fsck -n /dev/vx/dsk/<dg>/<vol>
‚úÖ 8Ô∏è‚É£ Check Service Group Operations
Switch service group:

bash
Copy
Edit
hagrp -switch <groupname> -to <node>
‚úîÔ∏è Ensure it moves cleanly and comes online

Online/Offline test:

bash
Copy
Edit
hagrp -offline <groupname> -sys <node>
hagrp -online <groupname> -sys <node>
‚úÖ 9Ô∏è‚É£ Review Logs
Cluster logs:

bash
Copy
Edit
tail -f /var/VRTSvcs/log/engine_A.log
Fencing logs (if configured):

bash
Copy
Edit
tail -f /var/VRTSvxfen/log/vxfen.log
üìå Health Check Summary Table
Check	Command
LLT links	lltstat -nvv
GAB ports	gabconfig -a
Fencing status	vxfenadm -d
Cluster status (HAD)	hastatus -sum
Disk group/volumes	vxdg list / vxdisk -o alldgs list
CVM cluster status	vxclustadm -m vcs nidmap
CFS mount status	`mount
Service group switch test	hagrp -switch
Log review	tail -f /var/VRTSvcs/log/engine_A.log



rpm -qa | grep -i vrts


[root@npe-esa-pfa-w2a-rp1 infoscale_7.4.2_base]# ll
total 2795936
-rw-r-----. 1 root root      95793 Jun 20 11:58 cpi-Patch-7.4.2.4000.tar.gz
-rw-r-----. 1 root root 1228424669 Jun 20 11:58 infoscale-rhel8_x86_64-Patch-7.4.2.5600.tar.gz
-rw-r-----. 1 root root 1634510709 Jun 20 02:48 Veritas_InfoScale_7.4.2_RHEL.tar.gz
[root@npe-esa-pfa-w2a-rp1 infoscale_7.4.2_base]#

[root@node01 ~]# cd /root/vcs_files/


scp -p npe-esa-pfa-w2a-rp1:/repos/infoscale_7.4.2_base/Veritas_InfoScale_7.4.2_RHEL.tar.gz .
scp -p npe-esa-pfa-w2a-rp1:/repos/infoscale_7.4.2_base/infoscale-rhel8_x86_64-Patch-7.4.2.5600.tar.gz .
scp -p npe-esa-pfa-w2a-rp1:/repos/infoscale_7.4.2_base/cpi-Patch-7.4.2.4000.tar.gz .

swap 12GB
/ 50GB
/tmp 15GB
/var 20GB
/var/log 15GB


lvextend -r -L 15G -n /dev/mapper/vg_01-LogVol02
lvextend -r -L +15G -n /dev/mapper/vg_01-LogVol02


free -h
swapoff -v /dev/mapper/vg_01-lv_swap
lvextend -L 13GB /dev/mapper/vg_01-lv_swap
mkswap /dev/mapper/vg_01-lv_swap
swapon -av
free -h


  184  20/06/25 03:07:03 swapoff -v /dev/mapper/vg_01-lv_swap
  185  20/06/25 03:07:06 free -h
  186  20/06/25 03:07:38 lvextend -L +8GB /dev/mapper/vg_01-lv_swap
  187  20/06/25 03:07:50 mkswap /dev/mapper/vg_01-lv_swap
  188  20/06/25 03:07:56 swapon -av
  189  20/06/25 03:08:01 free -h
  190  20/06/25 03:08:11 free -m
  191  20/06/25 03:08:15 free -g
  192  20/06/25 03:08:26 swapoff -v /dev/mapper/vg_01-lv_swap
  193  20/06/25 03:08:30 free -h
  194  20/06/25 03:08:39 lvextend -L +1GB /dev/mapper/vg_01-lv_swap
  195  20/06/25 03:08:45 mkswap /dev/mapper/vg_01-lv_swap
  196  20/06/25 03:08:54 swapon -av
  197  20/06/25 03:08:58 free -h
  198  20/06/25 03:09:16 vgs
  199  20/06/25 03:10:30 df -Th
  200  20/06/25 03:11:07 lvextend -r -L 15G -n /dev/mapper/vg_01-LogVol02
  201  20/06/25 03:11:14 df -Th
  202  20/06/25 03:11:23 gs
  203  20/06/25 03:11:25 vgs
  204  20/06/25 03:11:52 lvextend -r -L 45G -n  /dev/mapper/vg_01-LogVol00
  205  20/06/25 03:12:00 df -Th
  206  20/06/25 03:12:40 lvextend -r -L 20G -n  /dev/mapper/vg_01-LogVol03
  207  20/06/25 03:12:42 vgs
  208  20/06/25 03:12:46 df -Th
  209  20/06/25 03:13:13 lvextend -r -L 15G -n  /dev/mapper/vg_01-LogVol04
  210  20/06/25 03:13:16 df -Th
  211  20/06/25 03:13:17 vgs
  212  20/06/25 03:13:42 history
[root@node01 ~]#


lvextend -r -L 15G -n /dev/mapper/vg_01-LogVol02
lvextend -r -L 45G -n  /dev/mapper/vg_01-LogVol00
lvextend -r -L 20G -n  /dev/mapper/vg_01-LogVol03
lvextend -r -L 15G -n  /dev/mapper/vg_01-LogVol04
df -Th ; vgs


scp -p npe-esa-pfa-w2a-rp1:/repos/infoscale_7.4.2_base/Veritas_InfoScale_7.4.2_RHEL.tar.gz .

Veritas InfoScale Enterprise Install did not complete successfully

VRTSvxvm rpm failed to install on node01
VRTSvxvm rpm failed to install on node02

rpms/patches failed to install. Do you want to exit installer? [y,n,q] (y)

Refer to the installer logs for details and correct the settings. Continue with the installer again, or try to install the
packages/patches manually.

installer log files, summary file, and response file are saved at:

        /opt/VRTS/install/logs/installer-202506200348VyM

Would you like to view the summary file? [y,n,q] (n)

[root@node01 rhel8_x86_64]# cat /opt/VRTS/install/logs/installer-202506200348VyM/install.VRTSvxvm.node01
Verifying packages...
Preparing packages...
VRTSvxvm-7.4.2.0000-RHEL8.x86_64
Installing and Enabling VxVM systemd-services
creating VxVM device nodes under /dev
ERROR: No appropriate modules found. Error in loading module "vxdmp". See documentation.
warning: %post(VRTSvxvm-7.4.2.0000-RHEL8.x86_64) scriptlet failed, exit status 1

[root@node01 rhel8_x86_64]# cat /opt/VRTS/install/logs/installer-202506200348VyM/install.VRTSvxvm.node02
warning: /opt/VRTStmp/installer-202506200348VyM/VRTSvxvm-7.4.2.0000-RHEL8.x86_64.rpm: Header V4 RSA/SHA1 Signature, key ID cc633953: NOKEY
Verifying packages...
Preparing packages...
VRTSvxvm-7.4.2.0000-RHEL8.x86_64
Installing and Enabling VxVM systemd-services
creating VxVM device nodes under /dev
ERROR: No appropriate modules found. Error in loading module "vxdmp". See documentation.
warning: %post(VRTSvxvm-7.4.2.0000-RHEL8.x86_64) scriptlet failed, exit status 1
[root@node01 rhel8_x86_64]#

[root@node01 rhel8_x86_64]# cat /opt/VRTS/install/logs/installer-202506200348VyM/installer-202506200348VyM.summary
installer Summary

System verification checks completed

The following notes were discovered on the systems:
CPI NOTE V-9-30-1021 The system information on node01:
        Operating system: Linux RHEL 8.10 x86_64
        CPU number: 12
        CPU speed: 2596 MHz
        Memory size: 31857 MB
        Swap size: 13311 MB
CPI NOTE V-9-30-1021 The system information on node02:
        Operating system: Linux RHEL 8.10 x86_64
        CPU number: 12
        CPU speed: 2297 MHz
        Memory size: 31857 MB
        Swap size: 13311 MB

The following warnings were discovered on the systems:
CPI WARNING V-9-40-1418 Kernel Release 4.18.0-553.50.1.el8_10.x86_64 is detected on node01, which is not recognizable by the installer. It is strongly recommended to check it on SORT (https://sort.veritas.com) before continue.
CPI WARNING V-9-40-1418 Kernel Release 4.18.0-553.50.1.el8_10.x86_64 is detected on node02, which is not recognizable by the installer. It is strongly recommended to check it on SORT (https://sort.veritas.com) before continue.

The following checks and actions were performed on the systems:
CPI CHECK V-9-20-1139 System time on node01 is '02:50:10:20:06:2025'
CPI CHECK V-9-20-1139 System time on node02 is '02:50:10:20:06:2025'

Veritas InfoScale Enterprise Install did not complete successfully
VRTSvxvm rpm failed to install on node01
VRTSvxvm rpm failed to install on node02

The All package set was selected for installation



The following rpms were installed on node01:

        VRTSperl
        VRTSpython
        VRTSvlic
        VRTSspt
        VRTSveki
        VRTSaslapm
        VRTSvxfs
        VRTSfsadv
        VRTSllt
        VRTSgab
        VRTSvxfen
        VRTSamf
        VRTSvcs
        VRTScps
        VRTSvcsag
        VRTSvcsea
        VRTSdbed
        VRTSglm
        VRTScavf
        VRTSgms
        VRTSodm
        VRTSdbac
        VRTSsfmh
        VRTSvbs
        VRTSsfcpi
        VRTSvcswiz


The following rpms failed to install on node01:

        VRTSvxvm


The All package set was selected for installation



The following rpms were installed on node02:

        VRTSperl
        VRTSpython
        VRTSvlic
        VRTSspt
        VRTSveki
        VRTSaslapm
        VRTSvxfs
        VRTSfsadv
        VRTSllt
        VRTSgab
        VRTSvxfen
        VRTSamf
        VRTSvcs
        VRTScps
        VRTSvcsag
        VRTSvcsea
        VRTSdbed
        VRTSglm
        VRTScavf
        VRTSgms
        VRTSodm
        VRTSdbac
        VRTSsfmh
        VRTSvbs
        VRTSsfcpi
        VRTSvcswiz


The following rpms failed to install on node02:

        VRTSvxvm


[root@node01 rhel8_x86_64]#




[root@pl-p1-aux-05 ~]# rpm -qa | grep -i vrts
VRTSspt-7.4.2.1400-0021_RHEL8.noarch
VRTScavf-7.4.2.4700-GENERIC.x86_64
VRTSperl-5.30.0.5-RHEL8.x86_64
VRTSvcsea-7.4.2.3100-RHEL8.x86_64
VRTSvcs-7.4.2.3600-RHEL8.x86_64
VRTSvcswiz-7.4.2.2100-RHEL8.x86_64
VRTSgab-7.4.2.3200-RHEL8.x86_64
VRTSvxfs-7.4.2.5200-RHEL8.x86_64
VRTSvcsag-7.4.2.3600-RHEL8.x86_64
VRTSveki-7.4.2.3700-RHEL8.x86_64
VRTSamf-7.4.2.3500-RHEL8.x86_64
VRTSgms-7.4.2.4800-RHEL8.x86_64
VRTSsfmh-7.4.2.1100-0.x86_64
VRTSpython-3.9.2.0_1-RHEL8.x86_64
VRTSllt-7.4.2.3600-RHEL8.x86_64
VRTSdbed-7.4.2.2300-RHEL.x86_64
VRTSaslapm-7.4.2.5300-RHEL8.x86_64
VRTScps-7.4.2.3400-RHEL8.x86_64
VRTSdbac-7.4.2.2600-RHEL8.x86_64
VRTSvxvm-7.4.2.5301-RHEL8.x86_64
VRTSvxfen-7.4.2.3200-RHEL8.x86_64
VRTSsfcpi-7.4.2.3000-GENERIC.noarch
VRTSfsadv-7.4.2.4800-RHEL8.x86_64
VRTSvbs-7.4.2.0000-GA_Linux.x86_64
VRTSodm-7.4.2.5200-RHEL8.x86_64
VRTSvlic-4.01.742.300-RHEL8.x86_64
VRTSglm-7.4.2.4800-RHEL8.x86_64
[root@pl-p1-aux-05 ~]#


[root@node01 rhel8_x86_64]# cat docs/readme.txt

Veritas InfoScale‚Ñ¢ 7.4.2 Readme
=============================

* Important release information

* Installing Veritas InfoScale products

* Restarting the installer after a failed network connection


Important release information
=============================

Documents for this release are available on the SORT website
(https://sort.veritas.com/documents). They are no longer available with
the tarball.

Review the following information before installing the product:

* For information on system requirements, supported operating systems,
  and database versions for various components of Veritas InfoScale
  product suite, see the Veritas InfoScale Release Notes.

* For important updates regarding this release, look for the
  Updates, Patches, and Late Breaking News (LBN) for Veritas InfoScale 7.4.2 for UNIX (AIX, Linux, Solaris) at:
  https://www.veritas.com/content/support/en_US/DocumentBrowsing.html?product=InfoScale%20%26%20Storage%20Foundation

* For the latest patches available for this release, visit:
  https://sort.veritas.com/patches

* For the Ansible modules, playbook templates, and manual
  for using Ansible in an InfoScale environment visit:
  https://sort.veritas.com/utility/ansible

* For the latest information on supported hardware, see the
  Veritas InfoScale 7.4.2 Hardware Compatibility List (HCL) - AIX, Linux, Solaris:
  https://www.veritas.com/content/support/en_US/DocumentBrowsing.html?product=InfoScale%20%26%20Storage%20Foundation

  The hardware compatibility list contains information about supported
  hardware and is updated regularly.

* For the latest information on supported software, visit:
  https://www.veritas.com/content/support/en_US/DocumentBrowsing.html?product=InfoScale%20%26%20Storage%20Foundation
  Look for the following documents:
                 * Veritas InfoScale 7.4.2 Software Compatibility List - AIX
                 * Veritas InfoScale 7.4.2 Software Compatibility List - Linux
                 * Veritas InfoScale 7.4.2 Software Compatibility List - Solaris

  The software compatibility list summarizes each Veritas InfoScale
  product stack and the product features, operating system versions, and
  third-party products it supports.

* For detailed information on installation, see the Veritas InfoScale
  Installation Guide.


Installing Veritas InfoScale products
=====================================

1. Review the installation and system requirements.

2. Install or upgrade to the supported operating system.

3. Veritas InfoScale products are installed under /opt. Ensure that
   the directory /opt has enough free space and proper permission.

4. Download and uncompress the Veritas InfoScale software or mount the
   ISO image.

   Note: Download the files to a directory that does not contain any previous
   release of Veritas InfoScale products or maintenance pack.

5. Verify the compatibility of the system for installation. If any
   issues are reported, fix it.

   # ./installer -precheck.

6. For remote installation or installation on multiple systems, set up
   rsh or ssh.

   Note: The Veritas InfoScale installer can configure ssh and rsh, if required.


7. Install the Veritas InfoScale software.

   # ./installer.


Restarting the installer after a failed network connection
==========================================================

If an installation is aborted because of a failed network connection,
restarting the installer will detect the previous installation. The
installer prompts to resume the installation. If you choose to resume the
installation, the installer proceeds from the point where the
installation aborted. If you choose not to resume, the installation
starts from the beginning.[root@node01 rhel8_x86_64]#

/root/vcs_files/dvd1-redhatlinux/rhel8_x86_64


                                          Veritas InfoScale Enterprise 7.4.2 Install Program
                                                   node01 node02

Logs are being written to /opt/VRTStmp/installer-202506201112JWB while installer is in progress

    Verifying systems: 100%

    Estimated time remaining: (mm:ss) 0:00                                                                                   8 of 8

    Checking system communication ............................................................................................ Done
    Checking release compatibility ........................................................................................... Done
    Checking installed product ............................................................................................... Done
    Checking platform version ................................................................................................ Done
    Checking prerequisite patches and rpms ................................................................................... Done
    Checking file system free space .......................................................................................... Done
    Checking configured component ............................................................................................ Done
    Performing product prechecks ............................................................................................. Done

System verification checks completed

The following warnings were discovered on the systems:

CPI WARNING V-9-30-1148 InfoScale Enterprise version 7.4.2.0000 is already installed on node01

CPI WARNING V-9-40-1418 Kernel Release 4.18.0-553.50.1.el8_10.x86_64 is detected on node01, which is not recognizable by the
installer. It is strongly recommended to check it on SORT (https://sort.veritas.com) before continue.

CPI WARNING V-9-30-1148 InfoScale Enterprise version 7.4.2.0000 is already installed on node02

CPI WARNING V-9-40-1418 Kernel Release 4.18.0-553.50.1.el8_10.x86_64 is detected on node02, which is not recognizable by the
installer. It is strongly recommended to check it on SORT (https://sort.veritas.com) before continue.

Do you want to continue? [y,n,q] (y)

No rpms or patches will be installed on all systems

The updates to VRTSaslapm package are released via the SORT web page: https://sort.veritas.com/asl. To make sure you have the latest
version of VRTSaslapm (for up to date ASLs and APMs), download and install the latest package from the SORT web page.

You are running this virtual machine under a VMware environment. You may access the cluster view for this virtual machine using the
vSphere client. To access the cluster view for the virtual machine using the vSphere client, log on to the vCenter Server through the
vSphere client, navigate to the virtual machine in the inventory view and click on the 'High Availability' tab.
You may also access the cluster view from a browser. To access the cluster view through a browser, open the URL below in a browser:

https://<VM_IP_or_Hostname>:5634/vcs/admin/application_health.html

Veritas InfoScale Enterprise cannot be started without configuration.

Run the '/opt/VRTS/install/installer -configure' command when you are ready to configure Veritas InfoScale Enterprise.

https://10.253.10.131:5634/vcs/admin/application_health.html

https://<VM_IP_or_Hostname>:5634/vcs/admin/application_health.html

Veritas InfoScale Enterprise cannot be started without configuration.

Run the '/opt/VRTS/install/installer -configure' command when you are ready to configure Veritas InfoScale Enterprise.


Checking online updates for Veritas InfoScale Enterprise 7.4.2

        No updates available for Veritas InfoScale Enterprise 7.4.2

        Visit https://sort.veritas.com for more information.

installer log files, summary file, and response file are saved at:

        /opt/VRTS/install/logs/installer-202506201112JWB

Would you like to view the summary file? [y,n,q] (n)
[root@node01 rhel8_x86_64]#


https://www.veritas.com/support/en_US/downloads/detail.REL319342#item2



INSTALLING THE PATCH
--------------------
Run the Installer script to automatically install the patch:
-----------------------------------------------------------
Please be noted that the installation of this P-Patch will cause downtime.

To install the patch perform the following steps on at least one node in the cluster:
1. Copy the patch infoscale-rhel8_x86_64-Patch-7.4.2.5600.tar.gz to /tmp
2. Untar infoscale-rhel8_x86_64-Patch-7.4.2.5600.tar.gz to /tmp/hf
    # mkdir /tmp/hf
    # cd /tmp/hf
    # gunzip /tmp/infoscale-rhel8_x86_64-Patch-7.4.2.5600.tar.gz
    # tar xf /tmp/infoscale-rhel8_x86_64-Patch-7.4.2.5600.tar
3. Install the hotfix(Please be noted that the installation of this P-Patch will cause downtime.)
    # pwd /tmp/hf
    # ./installVRTSinfoscale742P5600 [<host1> <host2>...]

You can also install this patch together with 7.4.2 base release using Install Bundles
1. Download this patch and extract it to a directory
2. Change to the Veritas InfoScale 7.4.2 directory and invoke the installer script
   with -patch_path option where -patch_path should point to the patch directory
    # ./installer -patch_path [<path to this patch>] [<host1> <host2>...]


Full patch
rolling upgrade

selected full patch...



INSTALLING THE PATCH
--------------------
1. Copy the hot-fix CPI_7.4.2_P40.pl to /tmp
2. Run installation scripts with the hot-fix
   Example:
   ./installer -require /tmp/CPI_7.4.2_P40.pl


Q2 2024 OS Patching along with Veritas upgrade on Prod AWS US-WEST BSCS Veritas Servers [7] (Active site).

VCS upgrade WEST:
===============
Patch location: /root/GTH/Q12024 ==> 10.6.167.46
The mentioned kernel (4.18.0-513.24.1) is qualified as per our sort site and require additional patch.

ÔÉò	InfoScale 7.4.2 Update 7 Cumulative Patch on RHEL8 Platform
ÔÉò	Latest CPI Patch

To install the above patch your command will be below:

# cd /root/veritas/Q12024
# tar xf infoscale-rhel8_x86_64-Patch-7.4.2.4900.tar.gz
# cd /root/veritas/Q12024/CPI/
# tar xf cpi-Patch-7.4.2.3800.tar.gz

For CP nodes:-

#cd /root/veritas/Q12024/
# ./installVRTSinfoscale742P4900 -require /root/veritas/Q12024/patches/CPI_7.4.2_P38.pl prd-esa-bsa-e1b-cp4
#cd /root/veritas/Q12024/
# ./installVRTSinfoscale742P4900 -require /root/veritas/Q12024/patches/CPI_7.4.2_P38.pl prd-esa-bsa-e1c-cp6
#cd /root/veritas/Q12024/
# ./installVRTSinfoscale742P4900 -require /root/veritas/Q12024/patches/CPI_7.4.2_P38.pl prd-esa-bsa-e1b-cp5

For VCS nodes:-
The HofFix [HF] is the recommended for the environment: (All nodes one after other except CP nodes)
vm-rhel8_x86_64-HotFix-7.4.2.4801:
#tar xf vm-rhel8_x86_64-HotFix-7.4.2.4801.tar.gz

Select the appropriate RPMs for your system, and upgrade to the new patch.
    # rpm -Uhv VRTSvxvm-7.4.2.4801-RHEL7.x86_64.rpm

After completing above rpm on all the nodes then start actual VCS upgrade.
#cd /root/veritas/Q12024/
# ./installVRTSinfoscale742P4900 -require /root/veritas/Q12024/patches/CPI_7.4.2_P38.pl prd-esa-bsa-e1b-a09  prd-esa-bsa-e1c-a10 prd-esa-bsa-e1b-a11 prd-esa-bsa-e1c-a12

Once VCS upgrade done, then do OS patching. 
‚Ä¢	Unix team proceeds with patching of above servers using below commands.

Actual Patching


For RHEL 8.9

#yum clean all; yum --disablerepo="*" --enablerepo="rhel-8-baseos" --enablerepo="rhel-8-appstream" --exclude="*openjdk*" update

‚Ä¢	Once OS patching done, reboot servers one by one and verify OS version, VCS cluster status and replication status. 

‚Ä¢	Determine the CVM master :

[root@prd-esa-bsa-w2a-a11 ~]# vxdctl -c mode
mode: enabled: cluster active - SLAVE
master: prd-esa-bsa-w2b-a10
[root@prd-esa-bsa-w2a-a11 ~]#

‚Ä¢	Start VVR replication 

o	Trigger below cmd for doing autosync.
# vradmin -g datadg1 -a resumerep data_rvg1
# vradmin -g workdg1 -a resumerep work_rvg1

‚Ä¢	Verify replication status :
# vradmin -g workdg1 repstatus work_rvg1; vradmin -g datadg1 repstatus data_rvg1

‚Ä¢	Check the vradmin status. 
# vradmin -g workdg1 repstatus work_rvg1; vradmin -g datadg1 repstatus data_rvg1

‚Ä¢	Make sure the VVR replications started automatically, and Data status is ‚Äòup-to‚Äôdate‚Äô





























[root@node01 vcs_files]# history
    1  17/06/25 14:44:49 uname -r
    2  17/06/25 14:45:05 yum --disablerepo='*' --enablerepo "rhel-8-appstream" --enablerepo "rhel-8-baseos" --exclude="*openjdk*" install ipa-client -y
    3  17/06/25 14:49:51 wget http://10.158.108.81/install/scripts/postbuild_pllab.sh && chmod 700 postbuild_pllab.sh
    4  17/06/25 14:49:59 ls -lrth
    5  17/06/25 14:50:07 ./postbuild_pllab.sh node01
    6  17/06/25 14:57:20 systemctl status sssd
    7  17/06/25 14:57:24 ifconfig -a
    8  17/06/25 15:25:45 exit
    9  17/06/25 15:27:37 id -a edpyxx01@lab
   10  17/06/25 15:27:40 exit
   11  17/06/25 15:40:14 host node01
   12  17/06/25 15:40:21 host node01-traf
   13  17/06/25 15:40:29 ifconfig -a
   14  17/06/25 17:55:16 cd /root
   15  17/06/25 17:55:18 ls -lrth
   16  17/06/25 17:55:45 telnet omi1nmnet.px.t-mobile.com 383
   17  17/06/25 17:56:18 vi /etc/hosts
   18  17/06/25 17:56:36 telnet omi1nmnet.px.t-mobile.com 383
   19  17/06/25 17:56:49 cat /etc/hosts
   20  17/06/25 17:57:52 ping 10.253.227.21
   21  17/06/25 17:58:06 exit
   22  17/06/25 18:08:30 netstat -nr
   23  17/06/25 18:08:59 cat /etc/hosts
   24  17/06/25 18:09:13 telnet 10.255.35.21 383
   25  17/06/25 18:09:26 ping 10.255.35.21
   26  17/06/25 18:09:49 ping 10.253.227.21
   27  17/06/25 18:09:59 telnet 10.253.227.21 383
   28  17/06/25 18:12:24 telnet 10.156.224.20 383
   29  17/06/25 18:12:43 netstat -nr
   30  17/06/25 18:14:10 traceroute 10.253.227.21
   31  17/06/25 18:14:22 exit
   32  17/06/25 18:15:39 telnet omi1nmnet.tt.t-mobile.com 383
   33  17/06/25 18:15:51 telnet omi1nmnet.px.t-mobile.com 383
   34  17/06/25 18:17:21 ifconfig -a
   35  17/06/25 18:17:50 ping 10.253.10.1
   36  17/06/25 18:18:14 cat /etc/hosts
   37  17/06/25 18:18:42 telnet popomi03 383
   38  17/06/25 18:19:00 telnet popomi04 383
   39  17/06/25 18:19:15 telnet popomi05 383
   40  17/06/25 18:19:34 exit
   41  17/06/25 18:25:31 ifconfig -a
   42  17/06/25 18:26:06 netstat -nr
   43  17/06/25 18:26:41 ip route add 10.253.227.0/24 via 10.253.10.1 dev ens192
   44  17/06/25 18:26:45 netstat -nr
   45  17/06/25 18:27:10 telnet omi1nmnet.px.t-mobile.com 383
   46  17/06/25 18:28:46 ip route add 10.253.227.21/24 via 10.253.10.1 dev ens192
   47  17/06/25 18:28:58 ip route add 10.253.227.21/32 via 10.253.10.1 dev ens192
   48  17/06/25 18:29:03 telnet omi1nmnet.px.t-mobile.com 383
   49  17/06/25 18:29:09 telnet omi1nmnet.px.t-mobile.com 383
   50  17/06/25 18:29:23 nc -vz omi1nmnet.px.t-mobile.com 383
   51  17/06/25 18:29:39 nc -vvvz omi1nmnet.px.t-mobile.com 383
   52  17/06/25 18:30:09 telnet -vvv omi1nmnet.px.t-mobile.com
   53  17/06/25 18:30:15 telnet -vvv omi1nmnet.px.t-mobile.com 383
   54  17/06/25 18:31:52 exit
   55  17/06/25 19:01:48 cd /root
   56  17/06/25 19:01:50 ls -lrth
   57  17/06/25 19:02:27 rpm -ivh agent-client-collector-4.1.0-x86_64.rpm
   58  17/06/25 19:02:41 rpm -ivh agent-client-collector-4.2.1-x86_64_1.rpm
   59  17/06/25 19:03:31 vi /usr/lib/systemd/system/acc.service
   60  17/06/25 19:06:23 vi /etc/servicenow/agent-client-collector/acc.yml
   61  17/06/25 19:08:09 exit
   62  17/06/25 19:10:13 vi /etc/servicenow/agent-client-collector/acc.yml
   63  17/06/25 19:11:30 systemctl restart acc
   64  17/06/25 19:11:38 systemctl enable acc
   65  17/06/25 19:12:05 cat /etc/servicenow/agent-client-collector/acc.yml
   66  17/06/25 19:12:41 tail -f /var/log/servicenow/agent-client-collector/acc.log
   67  17/06/25 19:13:27 ls -lrth
   68  17/06/25 19:15:06 scp agent-client-collector-4.2.1-x86_64_1.rpm 10.253.10.132:/root
   69  17/06/25 19:15:18 scp agent-client-collector-4.2.1-x86_64_1.rpm 10.253.10.133:/root
   70  17/06/25 19:15:28 scp agent-client-collector-4.2.1-x86_64_1.rpm 10.253.10.134:/root
   71  17/06/25 19:16:33 exit
   72  17/06/25 18:50:14 ls -l /etc/pki/ca-trust/source/anchors
   73  17/06/25 20:23:48 exit
   74  18/06/25 03:15:06 history
   75  18/06/25 03:18:26 rpm -qa | grep -i nessus
   76  18/06/25 04:18:29 exit
   77  18/06/25 04:43:37 find / -name oainstall.sh
   78  18/06/25 04:45:17 history
   79  18/06/25 04:45:29 ls -l /etc/pki/ca-trust/source/anchors
   80  18/06/25 05:19:18 ip a
   81  18/06/25 05:19:20 lsblk
   82  18/06/25 05:19:22 route -n
   83  18/06/25 05:23:48 ls -l /etc/pki/ca-trust/source/anchors/
   84  18/06/25 05:24:08 chown root:root /etc/pki/ca-trust/source/anchors/*.pem
   85  18/06/25 05:24:11 ls -l /etc/pki/ca-trust/source/anchors/
   86  18/06/25 05:24:21 chmod 444 /etc/pki/ca-trust/source/anchors/*.pem
   87  18/06/25 05:24:36 update-ca-trust
   88  18/06/25 05:24:48 openssl verify /etc/pki/ca-trust/source/anchors/*.pem
   89  18/06/25 05:25:03 vi /etc/servicenow/agent-client-collector/acc.yml
   90  18/06/25 05:27:40 cat /etc/servicenow/agent-client-collector/acc.yml | grep -i wss://
   91  18/06/25 05:27:51 systemctl restart acc
   92  18/06/25 05:28:07 systemctl status acc
   93  18/06/25 05:28:12 tail -f /var/log/servicenow/agent-client-collector/acc.log
   94  18/06/25 05:28:34 date
   95  18/06/25 05:29:19 yum list curl net-tools bind-utils libnfnetlink libmnl libcap gmp libmnl sed gimp
   96  18/06/25 05:29:34 cat /etc/redhat-release
   97  18/06/25 05:29:59 yum --disablerepo="*" --enablerepo="rhel-8-baseos" --enablerepo="rhel-8-appstream" --exclude="*openjdk*" list curl net-tools bind-utils libnfnetlink libmnl libcap gmp libmnl sed gimp
   98  18/06/25 05:30:33 yum clean all
   99  18/06/25 05:30:39 yum repolist
  100  18/06/25 05:30:43 history
  101  18/06/25 05:33:30 ls /tmp/
  102  18/06/25 05:33:37 ls OA12.22_LIN.zip
  103  18/06/25 05:33:42 ll OA12.22_LIN.zip
  104  18/06/25 05:34:45 cat /etc/yum.repos.d/rhel-8.repo
  105  18/06/25 05:34:49 yum repolist
  106  18/06/25 05:34:58 nc -zv 10.6.167.46 80
  107  18/06/25 05:35:12 nc -zv 10.6.167.46 443
  108  18/06/25 05:35:57 vi /etc/yum.repos.d/rhel-8.repo
  109  18/06/25 05:36:44 vi /etc/yum.repos.d/custom-rhel-8-corp.repo
  110  18/06/25 05:36:50 yum clean all
  111  18/06/25 05:36:54 yum repolist
  112  18/06/25 05:37:01 yum list telnet
  113  18/06/25 05:43:21 yum list curl net-tools bind-utils libnfnetlink libmnl libcap gmp libmnl sed gimp
  114  18/06/25 05:43:37 yum install curl net-tools bind-utils libnfnetlink libmnl libcap gmp libmnl sed gimp
  115  18/06/25 05:44:31 update-ca-trust
  116  18/06/25 05:44:40 openssl verify /etc/pki/ca-trust/source/anchors/*.pem
  117  18/06/25 05:45:08 /opt/illumio_ven/illumio-ven-ctl status
  118  18/06/25 05:45:37 ls /opt/
  119  18/06/25 05:46:11 vi /etc/hosts
  120  18/06/25 05:47:54 cd /tmp/
  121  18/06/25 05:47:56 ls OA12.22_LIN.zip
  122  18/06/25 05:48:05 unzip OA12.22_LIN.zip
  123  18/06/25 05:48:22 cd OA12.22_LIN/
  124  18/06/25 05:48:24 ls
  125  18/06/25 05:50:13 ./oainstall.sh -i -a -srv omi1nmnet.t-mobile.com  -cs omi1nmnet.t-mobile.com
  126  18/06/25 05:50:25 chmod u+x oainstall.sh
  127  18/06/25 05:50:26 ls
  128  18/06/25 05:50:34 ./oainstall.sh -i -a -srv omi1nmnet.t-mobile.com  -cs omi1nmnet.t-mobile.com
  129  18/06/25 05:50:39 ll oainstall.sh
  130  18/06/25 05:52:54 pwd
  131  18/06/25 05:53:05 chmod +x oainstall.sh
  132  18/06/25 05:53:13 ./oainstall.sh -i -a -srv omi1nmnet.t-mobile.com  -cs omi1nmnet.t-mobile.com
  133  18/06/25 05:53:46 chmod +x ./scripts/oainstall_linux.sh
  134  18/06/25 05:53:53 ./oainstall.sh -i -a -srv omi1nmnet.t-mobile.com  -cs omi1nmnet.t-mobile.com
  135  18/06/25 05:54:40 chmod +x ./scripts/*.sh
  136  18/06/25 05:54:44 ./oainstall.sh -i -a -srv omi1nmnet.t-mobile.com  -cs omi1nmnet.t-mobile.com
  137  18/06/25 05:55:02 chmod +x ./scripts/*.sh
  138  18/06/25 05:55:09 chmod -R +x ./scripts/*.sh
  139  18/06/25 05:55:15 ./oainstall.sh -i -a -srv omi1nmnet.t-mobile.com  -cs omi1nmnet.t-mobile.com
  140  18/06/25 05:56:02 bash ./scripts/oainstall_Linux2.6_X64.sh
  141  18/06/25 05:56:15 ./oainstall.sh -i -a -srv omi1nmnet.t-mobile.com  -cs omi1nmnet.t-mobile.com
  142  18/06/25 05:56:53 chmod +x ./oasetup.sh
  143  18/06/25 05:57:01 ls
  144  18/06/25 05:58:43 find . -type f -name "*.sh" -exec chmod +x {} \;
  145  18/06/25 05:58:49 ./oainstall.sh -i -a -srv omi1nmnet.t-mobile.com  -cs omi1nmnet.t-mobile.com
  146  18/06/25 06:01:35 /opt/OV/bin/opcagt -status
  147  18/06/25 06:01:43 ls /opt/
  148  18/06/25 06:02:36 ./oainstall.sh -i -a -srv omi1nmnet.t-mobile.com  -cs omi1nmnet.t-mobile.com
  149  18/06/25 06:30:02 ls /opt/
  150  18/06/25 06:30:51 cat /var/opt/OV/log/oainstall.log | grep -i HPOvPerfMI
  151  18/06/25 06:42:12 /opt/OV/bin/opcagt -version
  152  18/06/25 06:42:19 ll /opt/OV/
  153  18/06/25 06:43:10 ./oainstall.sh -i -a -srv belomi01.unix.gsm1900.org -cs belomi01.unix.gsm1900.org
  154  18/06/25 06:45:00 ll /opt/OV/
  155  18/06/25 06:47:35 pwd
  156  18/06/25 06:47:37 ll
  157  18/06/25 06:47:45 ls packages/
  158  18/06/25 12:13:03 df -h
  159  18/06/25 12:13:18 pvs
  160  18/06/25 13:05:18 exit
  161  19/06/25 03:13:56 pvs
  162  19/06/25 03:13:58 vgs
  163  19/06/25 03:13:59 lvs
  164  19/06/25 03:17:40 cat /etc/passwd
  165  19/06/25 03:22:45 df -Th
  166  19/06/25 03:25:31 java -version
  167  19/06/25 06:16:35 useradd infaadm
  168  19/06/25 06:16:41 useradd tidal
  169  19/06/25 06:16:50 useradd auxemmsftp
  170  19/06/25 06:16:57 useradd auxadm
  171  19/06/25 06:17:10 passwd infaadm
  172  19/06/25 06:17:42 passwd tidal
  173  19/06/25 06:18:21 passwd auxemmsftp
  174  19/06/25 06:18:50 passwd auxadm
  175  20/06/25 03:35:13 ssh-keygen
  176  20/06/25 03:35:40 ssh-copy-id node02
  177  20/06/25 03:35:51 ssh node02
  178  20/06/25 03:52:47 ip a
  179  20/06/25 03:59:34 df -Th
  180  20/06/25 03:59:35 lsblk
  181  20/06/25 04:40:00 rpm -qa | grep VRTS
  182  20/06/25 04:41:04 rpm -qa | grep VRTSvxvm
  183  20/06/25 02:49:21 df -Th
  184  20/06/25 02:49:36 lsblk
  185  20/06/25 02:54:00 free -h
  186  20/06/25 03:02:02 pvs
  187  20/06/25 03:02:12 df -Th
  188  20/06/25 03:04:02 vgs
  189  20/06/25 03:05:20 cat /etc/fstab
  190  20/06/25 03:05:44 vgs
  191  20/06/25 03:06:08 free -h
  192  20/06/25 03:07:03 swapoff -v /dev/mapper/vg_01-lv_swap
  193  20/06/25 03:07:06 free -h
  194  20/06/25 03:07:38 lvextend -L +8GB /dev/mapper/vg_01-lv_swap
  195  20/06/25 03:07:50 mkswap /dev/mapper/vg_01-lv_swap
  196  20/06/25 03:07:56 swapon -av
  197  20/06/25 03:08:01 free -h
  198  20/06/25 03:08:11 free -m
  199  20/06/25 03:08:15 free -g
  200  20/06/25 03:08:26 swapoff -v /dev/mapper/vg_01-lv_swap
  201  20/06/25 03:08:30 free -h
  202  20/06/25 03:08:39 lvextend -L +1GB /dev/mapper/vg_01-lv_swap
  203  20/06/25 03:08:45 mkswap /dev/mapper/vg_01-lv_swap
  204  20/06/25 03:08:54 swapon -av
  205  20/06/25 03:08:58 free -h
  206  20/06/25 03:09:16 vgs
  207  20/06/25 03:10:30 df -Th
  208  20/06/25 03:11:07 lvextend -r -L 15G -n /dev/mapper/vg_01-LogVol02
  209  20/06/25 03:11:14 df -Th
  210  20/06/25 03:11:23 gs
  211  20/06/25 03:11:25 vgs
  212  20/06/25 03:11:52 lvextend -r -L 45G -n  /dev/mapper/vg_01-LogVol00
  213  20/06/25 03:12:00 df -Th
  214  20/06/25 03:12:40 lvextend -r -L 20G -n  /dev/mapper/vg_01-LogVol03
  215  20/06/25 03:12:42 vgs
  216  20/06/25 03:12:46 df -Th
  217  20/06/25 03:13:13 lvextend -r -L 15G -n  /dev/mapper/vg_01-LogVol04
  218  20/06/25 03:13:16 df -Th
  219  20/06/25 03:13:17 vgs
  220  20/06/25 03:13:42 history
  221  20/06/25 03:14:10 ll
  222  20/06/25 03:14:16 cd /root/
  223  20/06/25 03:14:16 ll
  224  20/06/25 03:14:45 mkdir vcs_files
  225  20/06/25 03:14:47 cd vcs_files/
  226  20/06/25 03:14:48 ll
  227  20/06/25 03:14:50 pwd
  228  20/06/25 03:15:07 ping npe-esa-pfa-w2a-rp1
  229  20/06/25 03:15:29 scp -p npe-esa-pfa-w2a-rp1:/repos/infoscale_7.4.2_base/Veritas_InfoScale_7.4.2_RHEL.tar.gz .
  230  20/06/25 03:16:05 ll
  231  20/06/25 03:16:27 tar -xvf Veritas_InfoScale_7.4.2_RHEL.tar.gz
  232  20/06/25 03:16:52 ll
  233  20/06/25 03:16:56 du -sh
  234  20/06/25 03:17:02 cd dvd1-redhatlinux
  235  20/06/25 03:17:04 ll
  236  20/06/25 03:17:08 cd rhel8_x86_64
  237  20/06/25 03:17:09 ll
  238  20/06/25 03:17:15 cd docs
  239  20/06/25 03:17:16 ll
  240  20/06/25 03:17:25 cat readme.txt | more
  241  20/06/25 03:18:49 cd ../
  242  20/06/25 03:18:52 ll
  243  20/06/25 03:19:00 ./installer -precheck
  244  20/06/25 04:05:59 ll /opt/VRTS/install/logs
  245  20/06/25 04:06:36 ls -lrta /opt/VRTS/install/logs/installer-202506200348VyM/
  246  20/06/25 04:07:02 cat /opt/VRTS/install/logs/installer-202506200348VyM/install.VRTSvxvm.node01
  247  20/06/25 04:07:37 cat /opt/VRTS/install/logs/installer-202506200348VyM/install.VRTSvxvm.node02
  248  20/06/25 04:14:49 cat /opt/VRTS/install/logs/installer-202506200348VyM/installer-202506200348VyM.summary
  249  20/06/25 05:26:50 host 10.159.176.215
  250  20/06/25 06:16:36 /opt/VRTSvcs/bin/hastatus -sum
  251  20/06/25 10:44:57 ip a
  252  20/06/25 10:48:58 nmtui
  253  20/06/25 10:50:05 ip a
  254  20/06/25 10:50:09 route -n
  255  20/06/25 10:50:19 ping 192.168.47.1
  256  20/06/25 10:53:15 ping 192.168.47.6
  257  20/06/25 10:53:19 ping 192.168.47.7
  258  20/06/25 10:59:22 lsblk
  259  20/06/25 10:59:49 cd /root/vcs_files/
  260  20/06/25 10:59:51 ll
  261  20/06/25 10:59:53 cd dvd1-redhatlinux/
  262  20/06/25 10:59:54 ll
  263  20/06/25 10:59:57 cd rhel8_x86_64
  264  20/06/25 10:59:59 ll
  265  20/06/25 11:00:16 ./installer -precheck
  266  20/06/25 11:08:13 ll
  267  20/06/25 11:08:23 cat docs/readme.txt
  268  20/06/25 11:08:42 pwd
  269  20/06/25 11:09:39 ./installer
  270  20/06/25 11:58:38 init 0
  271  20/06/25 17:46:29 ip a
  272  20/06/25 17:48:38 cd /root/vcs_files/
  273  20/06/25 17:48:39 ll
  274  20/06/25 17:48:54 pwd
  275  20/06/25 17:49:20 scp -p npe-esa-pfa-w2a-rp1:/repos/infoscale_7.4.2_base/infoscale-rhel8_x86_64-Patch-7.4.2.5600.tar.gz .
  276  20/06/25 17:51:20 scp -p npe-esa-pfa-w2a-rp1:/repos/infoscale_7.4.2_base/cpi-Patch-7.4.2.4000.tar.gz .
  277  20/06/25 17:51:28 ll
  278  20/06/25 17:51:43 tar -xvf infoscale-rhel8_x86_64-Patch-7.4.2.5600.tar.gz
  279  20/06/25 17:51:57 ll
  280  20/06/25 17:52:24 mkdir cpipatch
  281  20/06/25 17:52:36 mv cpi-Patch-7.4.2.4000.tar.gz cpipatch/
  282  20/06/25 17:52:38 ll
  283  20/06/25 17:52:40 cd cpipatch/
  284  20/06/25 17:52:41 ll
  285  20/06/25 17:52:47 tar -xvf cpi-Patch-7.4.2.4000.tar.gz
  286  20/06/25 17:52:50 ll
  287  20/06/25 17:52:54 cd ../
  288  20/06/25 17:52:55 ll
  289  20/06/25 17:53:01 cat README
  290  20/06/25 17:54:12 ll
  291  20/06/25 17:54:57 #./installVRTSinfoscale742P5600 node01 node02
  292  20/06/25 17:55:03 cat cpipatch/README
  293  20/06/25 17:55:27 ./installVRTSinfoscale742P5600 node01 node02
  294  20/06/25 18:29:53 rpm -qa | grep -i vrts
  295  20/06/25 18:30:03 cd cpipatch/
  296  20/06/25 18:30:04 ll
  297  20/06/25 18:30:06 cat README
  298  20/06/25 18:30:37 ll
  299  20/06/25 18:30:47 cd patches/
  300  20/06/25 18:30:47 ll
  301  20/06/25 18:34:10 #/opt/VRTS/install/installer -require
  302  20/06/25 18:34:12 pwd
  303  20/06/25 18:34:53 /opt/VRTS/install/installer -require /root/vcs_files/cpipatch/patches/CPI_7.4.2_P40.pl
  304  20/06/25 18:39:58 cd ../
  305  20/06/25 18:40:07 cd cpipatch/
  306  20/06/25 18:40:08 ll
  307  20/06/25 18:40:15 ll patches/
  308  20/06/25 18:40:21 cd ../../
  309  20/06/25 18:40:21 ll
  310  20/06/25 18:40:27 cd vcs_files/
  311  20/06/25 18:40:28 ll
  312  20/06/25 18:40:37 ll scripts/
  313  20/06/25 18:40:56 ll cpipatch/
  314  20/06/25 18:41:02 ll cpipatch/patches/
  315  20/06/25 18:41:03 ll
  316  20/06/25 18:41:40 cd cpipatch/patches/
  317  20/06/25 18:41:40 ll
  318  20/06/25 18:41:45 ./CPI_7.4.2_P40.pl
  319  20/06/25 18:42:18 /opt/VRTS/install/installer -version
  320  20/06/25 18:55:48 ll /opt/VRTS/install/.matrix/rhel8_x86_64-ga.json
  321  20/06/25 18:55:51 cd ../
  322  20/06/25 18:55:54 ../
  323  20/06/25 18:55:55 ll
  324  20/06/25 18:55:59 cd ../
  325  20/06/25 18:56:00 ll
  326  20/06/25 18:56:07 cd dvd1-redhatlinux/
  327  20/06/25 18:56:11 cd ../
  328  20/06/25 18:56:11 ll
  329  20/06/25 18:56:27 mkdir installVRTSinfoscale742P5600
  330  20/06/25 18:56:33 mkdir installVRTSinfoscale
  331  20/06/25 18:56:49 mv installVRTSinfoscale installVRTSinfoscale-Patch
  332  20/06/25 18:56:52 ll
  333  20/06/25 18:57:11 mv installVRTSinfoscale742P5600 README rpms scripts installVRTSinfoscale-Patch/
  334  20/06/25 18:57:13 ll
  335  20/06/25 18:57:41 history
[root@node01 vcs_files]#




Fencing configuration
     1)  Configure Coordination Point client based fencing
     2)  Configure disk based fencing
     3)  Configure majority based fencing

Select the fencing mechanism to be configured in this Application Cluster: [1-3,q,?] q
Are you sure you want to quit? [y,n] (y)

installer log files and summary file are saved at:

        /opt/VRTS/install/logs/installer-202506202143vcU

Would you like to view the summary file? [y,n,q] (n)

[root@node01 installVRTSinfoscale-Patch]#
[root@node01 installVRTSinfoscale-Patch]#
[root@node01 installVRTSinfoscale-Patch]# 

[root@node01 installVRTSinfoscale-Patch]# cat /opt/VRTS/install/logs/installer-202506202143vcU/installer-202506202143vcU.summary

 
Example: Stop Entire Cluster from One Node
bash
Copy
Edit
hastop -all
# Then on each node:
gabconfig -U
lltconfig -U
systemctl stop vxfen


[root@node01 installVRTSinfoscale-Patch]# /opt/VRTSvcs/bin/hastatus -sum
VCS ERROR V-16-1-10600 Cannot connect to VCS engine
VCS WARNING V-16-1-11046 Local system not available
[root@node01 installVRTSinfoscale-Patch]# gabconfig -U
GAB gabconfig ERROR V-15-2-25014 clients still registered
[root@node01 installVRTSinfoscale-Patch]# lltconfig -U
lltconfig: this will attempt to stop and reset LLT. Confirm (y/n)? y
lltconfig: this will attempt to stop and reset LLT. Confirm (y/n)? y
LLT lltconfig ERROR V-14-2-15121 LLT unconfigure aborted, unregister 4 port(s)
[root@node01 installVRTSinfoscale-Patch]#


[root@node02 ~]# /opt/VRTSvcs/bin/hastatus -sum
VCS ERROR V-16-1-10600 Cannot connect to VCS engine
VCS WARNING V-16-1-11046 Local system not available
[root@node02 ~]# gabconfig -U
GAB gabconfig ERROR V-15-2-25014 clients still registered
[root@node02 ~]# lltconfig -U
lltconfig: this will attempt to stop and reset LLT. Confirm (y/n)?
lltconfig: this will attempt to stop and reset LLT. Confirm (y/n)?
lltconfig: this will attempt to stop and reset LLT. Confirm (y/n)? y
LLT lltconfig ERROR V-14-2-15121 LLT unconfigure aborted, unregister 4 port(s)
[root@node02 ~]#



/opt/VRTSvcs/bin/hastop -all

[root@node02 ~]# /opt/VRTSvcs/bin/hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
[root@node02 ~]#


[root@node02 ~]# vxdctl -c mode
mode: enabled: cluster active - SLAVE
master: node01
[root@node02 ~]#


[root@pl-p1-aux-05 ~]# /opt/VRTSvcs/bin/hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  pl-p1-aux-05         RUNNING              0
A  pl-p1-aux-06         RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             pl-p1-aux-05         Y          N               ONLINE
B  cvm             pl-p1-aux-06         Y          N               ONLINE
B  vip_sg          pl-p1-aux-05         Y          N               OFFLINE
B  vip_sg          pl-p1-aux-06         Y          N               OFFLINE
B  vrts_vea_cfs_int_cfsmount1 pl-p1-aux-05         Y          N               ONLINE
B  vrts_vea_cfs_int_cfsmount1 pl-p1-aux-06         Y          N               ONLINE
B  vxfen           pl-p1-aux-05         Y          N               ONLINE
B  vxfen           pl-p1-aux-06         Y          N               ONLINE
[root@pl-p1-aux-05 ~]#



[root@pl-p1-aux-05 ~]# gabconfig -a
GAB Port Memberships
===============================================================
Port a gen   9cc001 membership 01
Port a gen   9cc001   jeopardy ;1
Port b gen   9cc006 membership 01
Port b gen   9cc006   jeopardy ;1
Port d gen   9cc005 membership 01
Port d gen   9cc005   jeopardy ;1
Port f gen   9cc011 membership 01
Port f gen   9cc011   jeopardy ;1
Port h gen   9cc004 membership 01
Port h gen   9cc004   jeopardy ;1
Port m gen   9cc008 membership 01
Port m gen   9cc008   jeopardy ;1
Port u gen   9cc00e membership 01
Port u gen   9cc00e   jeopardy ;1
Port v gen   9cc00a membership 01
Port v gen   9cc00a   jeopardy ;1
Port w gen   9cc00c membership 01
Port w gen   9cc00c   jeopardy ;1
Port y gen   9cc009 membership 01
Port y gen   9cc009   jeopardy ;1
[root@pl-p1-aux-05 ~]# systemctl status vxfen
‚óè vxfen.service - VERITAS I/O Fencing (VxFEN)
   Loaded: loaded (/opt/VRTSvcs/vxfen/bin/vxfen; enabled; vendor preset: disabled)
   Active: active (exited) since Sun 2025-06-08 07:13:34 PDT; 1 weeks 5 days ago
  Process: 21154 ExecStart=/opt/VRTSvcs/vxfen/bin/vxfen start 2>&1 (code=exited, status=0/SUCCESS)
    Tasks: 0 (limit: 1646438)
   Memory: 0B
   CGroup: /system.slice/vxfen.service

Warning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.
[root@pl-p1-aux-05 ~]# vxfenadm -d

I/O Fencing Cluster Information:
================================

 Fencing Protocol Version: 201
 Fencing Mode: SCSI3
 Fencing SCSI3 Disk Policy: dmp
 Cluster Members:

        * 0 (pl-p1-aux-05)
          1 (pl-p1-aux-06)

 RFSM State Information:
        node   0 in state  8 (running)
        node   1 in state  8 (running)

[root@pl-p1-aux-05 ~]# vxfenadm -p
vxfenadm: usage:
        -a -k|K <key> -f <disk_file>



üìå Summary of Commands:
Purpose	Command
Check GAB fencing port status	gabconfig -a
Verify vxfen service status	systemctl status vxfen
List registered coordination disks	vxfenadm -d
View current node‚Äôs fencing key	vxfenadm -p
Review fencing logs	tail -f /var/VRTSvcs/log/vxfen.log
Check SCSI PR on disk (optional)	sg_persist --in --read-reservation /dev/mapper/mpathX


[root@pl-p1-aux-05 ~]# cat /etc/llttab
set-node pl-p1-aux-05
set-cluster 50376
link ens4f0 eth-64:51:06:ef:70:e8 - ether - -
link ens5f0 eth-64:51:06:ef:79:10 - ether - -
link-lowpri bond0 bond0 - ether - -
[root@pl-p1-aux-05 ~]#
[root@pl-p1-aux-05 ~]#


[root@pl-p1-aux-05 ~]# lltstat -nvv
LLT node information:
    Node                 State    Link  Status  Address
   * 0 pl-p1-aux-05      OPEN
                                  ens4f0   UP      64:51:06:EF:70:E8
                                  ens5f0   UP      64:51:06:EF:79:10
                                  bond0   UP      28:80:23:AE:64:9C
     1 pl-p1-aux-06      OPEN
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   UP      28:80:23:AE:46:04
     2                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
     3                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
     4                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
     5                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
     6                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
     7                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
     8                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
     9                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    10                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    11                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    12                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    13                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    14                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    15                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    16                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    17                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    18                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    19                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    20                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    21                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    22                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    23                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    24                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    25                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    26                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    27                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    28                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    29                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    30                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    31                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    32                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    33                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    34                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    35                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    36                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    37                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    38                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    39                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    40                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    41                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    42                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    43                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    44                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    45                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    46                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    47                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    48                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    49                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    50                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    51                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    52                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    53                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    54                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    55                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    56                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    57                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    58                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    59                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    60                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    61                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    62                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    63                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    64                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    65                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    66                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    67                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    68                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    69                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    70                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    71                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    72                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    73                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    74                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    75                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    76                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    77                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    78                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    79                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    80                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    81                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    82                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    83                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    84                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    85                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    86                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    87                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    88                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    89                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    90                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    91                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    92                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    93                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    94                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    95                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    96                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    97                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    98                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    99                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    100                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    101                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    102                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    103                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    104                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    105                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    106                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    107                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    108                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    109                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    110                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    111                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    112                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    113                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    114                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    115                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    116                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    117                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    118                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    119                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    120                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    121                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    122                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    123                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    124                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    125                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    126                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
    127                   CONNWAIT
                                  ens4f0   DOWN
                                  ens5f0   DOWN
                                  bond0   DOWN
[root@pl-p1-aux-05 ~]#


                                         Veritas InfoScale Enterprise 7.4.2 Configure Program
                                                   node01 node02

Fencing configuration
     1)  Configure Coordination Point client based fencing
     2)  Configure disk based fencing
     3)  Configure majority based fencing

Select the fencing mechanism to be configured in this Application Cluster: [1-3,q,?] 3

Majority-based I/O fencing has limited functionality. In case of a split brain, where none of the sub-clusters have a majority or
where the sub-cluster having majority is hung, all nodes in the cluster may panic. Refer to VCS Administrator's Guide for more
details.

Do you want to continue? [y,n,q] (n) y

Does your storage environment support SCSI3 PR? [y,n,q,?] n

Installer will stop VCS before applying fencing configuration. To make sure VCS shuts down successfully, unfreeze any frozen service
group and unmount the mounted file systems in the cluster.

HAD and all the applications will be stopped. Do you want to stop VCS and all its applications and apply fencing configuration on all
nodes at this point? [y,n,q] (y)
    Stopping VCS on node02 ........................................................................................ Done
    Stopping vxglm on node02 ...................................................................................... Done
    Stopping vxodm on node02 ...................................................................................... Done
    Stopping vxgms on node02 ...................................................................................... Done
    Stopping vxfen on node02 ...................................................................................... Done
    Stopping gab on node02 ........................................................................................ Done
    Stopping VCS on node01 ........................................................................................ Done
    Stopping vxglm on node01 ...................................................................................... Done
    Stopping vxodm on node01 ...................................................................................... Done
    Stopping vxgms on node01 ...................................................................................... Done
    Stopping vxfen on node01 ...................................................................................... Done
    Stopping gab on node01 ........................................................................................ Done
    Updating /etc/vxfenmode file on node01 ........................................................................ Done
    Updating /etc/vxenviron file on node01 ........................................................................ Done
    Updating /etc/sysconfig/vxfen file on node01 .................................................................. Done
    Updating /etc/llttab file on node01 ........................................................................... Done
    Updating /etc/vxfenmode file on node02 ........................................................................ Done
    Updating /etc/vxenviron file on node02 ........................................................................ Done
    Updating /etc/sysconfig/vxfen file on node02 .................................................................. Done
    Updating /etc/llttab file on node02 ........................................................................... Done
    Starting gab on node01 ........................................................................................ Done
    Starting gab on node02 ........................................................................................ Done
    Starting vxfen on node01 ...................................................................................... Done
    Starting vxfen on node02 ...................................................................................... Done
    Starting vxgms on node01 ...................................................................................... Done
    Starting vxgms on node02 ...................................................................................... Done
    Starting vxodm on node01 ...................................................................................... Done
    Starting vxodm on node02 ...................................................................................... Done
    Starting vxglm on node01 ...................................................................................... Done
    Starting vxglm on node02 ...................................................................................... Done
    Updating main.cf with fencing ........................................................................................... Done
    Starting VCS on node01 ........................................................................................ Done
    Starting VCS on node02 ........................................................................................ Done
    I/O Fencing configuration ............................................................................................... Done

I/O Fencing configuration completed successfully

The updates to VRTSaslapm package are released via the SORT web page: https://sort.veritas.com/asl. To make sure you have the latest
version of VRTSaslapm (for up to date ASLs and APMs), download and install the latest package from the SORT web page.

You are running this virtual machine under a VMware environment. You may access the cluster view for this virtual machine using the
vSphere client. To access the cluster view for the virtual machine using the vSphere client, log on to the vCenter Server through the
vSphere client, navigate to the virtual machine in the inventory view and click on the 'High Availability' tab.
You may also access the cluster view from a browser. To access the cluster view through a browser, open the URL below in a browser:

https://<VM_IP_or_Hostname>:5634/vcs/admin/application_health.html



[root@node02 ~]# /opt/VRTSvcs/bin/hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
[root@node02 ~]# vxfenadm -d

I/O Fencing Cluster Information:
================================

 Fencing Protocol Version: 201
 Fencing Mode: MAJORITY
 Cluster Members:

          0 (node01)
        * 1 (node02)

 RFSM State Information:
        node   0 in state  8 (running)
        node   1 in state  8 (running)

[root@node02 ~]# gabconfig -a
GAB Port Memberships
===============================================================
Port a gen   2f0101 membership 01
Port a gen   2f0101   jeopardy ;1
Port b gen   2f0104 membership 01
Port b gen   2f0104   jeopardy ;1
Port d gen   2f0107 membership 01
Port d gen   2f0107   jeopardy ;1
Port f gen   2f0115 membership 01
Port f gen   2f0115   jeopardy ;1
Port h gen   2f010a membership 01
Port h gen   2f010a   jeopardy ;1
Port m gen   2f010c membership 01
Port m gen   2f010c   jeopardy ;1
Port u gen   2f0112 membership 01
Port u gen   2f0112   jeopardy ;1
Port v gen   2f010e membership 01
Port v gen   2f010e   jeopardy ;1
Port w gen   2f0110 membership 01
Port w gen   2f0110   jeopardy ;1
Port y gen   2f010d membership 01
Port y gen   2f010d   jeopardy ;1
[root@node02 ~]# cat /etc/llttab
set-node node02
set-cluster 35461
set-timer sendhbcap:1800
link ens161 eth-00:50:56:ac:15:c4 - ether - -
link-lowpri ens224 eth-00:50:56:ac:26:92 - ether - -
[root@node02 ~]# vxdctl -c mode
mode: enabled: cluster active - SLAVE
master: node01
[root@node02 ~]#



[root@node01 ~]# lltstat -nvv
LLT node information:
    Node                 State    Link  Status  Address
   * 0 node01  OPEN
                                  ens161   UP      00:50:56:AC:F7:BA
                                  ens224   UP      00:50:56:AC:6C:77
     1 node02  OPEN
                                  ens161   DOWN
                                  ens224   UP      00:50:56:AC:26:92


[root@node02 ~]# lltstat -nvv
LLT node information:
    Node                 State    Link  Status  Address
     0 node01  OPEN
                                  ens161   DOWN
                                  ens224   UP      00:50:56:AC:6C:77
   * 1 node02  OPEN
                                  ens161   UP      00:50:56:AC:15:C4
                                  ens224   UP      00:50:56:AC:26:92
     2                   CONNWAIT
                                  ens161   DOWN
                                  ens224   DOWN
     3                   CONNWAIT

[root@node01 ~]# cat /etc/llttab
set-node node01
set-cluster 35461
set-timer sendhbcap:1800
link ens161 eth-00:50:56:ac:f7:ba - ether - -
link-lowpri ens224 eth-00:50:56:ac:6c:77 - ether - -
[root@node01 ~]#


[root@node02 ~]# cat /etc/llttab
set-node node02
set-cluster 35461
set-timer sendhbcap:1800
link ens161 eth-00:50:56:ac:15:c4 - ether - -
link-lowpri ens224 eth-00:50:56:ac:26:92 - ether - -
[root@node02 ~]#


[root@node02 ~]# ping -I ens161 192.168.47.5
PING 192.168.47.5 (192.168.47.5) from 192.168.47.6 ens161: 56(84) bytes of data.
From 192.168.47.6 icmp_seq=1 Destination Host Unreachable
From 192.168.47.6 icmp_seq=2 Destination Host Unreachable
From 192.168.47.6 icmp_seq=3 Destination Host Unreachable


[root@node01 ~]# ping -I ens161 192.168.47.6
PING 192.168.47.6 (192.168.47.6) from 192.168.47.5 ens161: 56(84) bytes of data.
From 192.168.47.5 icmp_seq=1 Destination Host Unreachable
From 192.168.47.5 icmp_seq=2 Destination Host Unreachable
From 192.168.47.5 icmp_seq=3 Destination Host Unreachable


üìå üìñ Step-by-Step: Create Shared Mount Point on 1TB Shared Disk (VxFS CFS)
üìå 1Ô∏è‚É£ Verify the Shared Disk
On one node:

bash
Copy
Edit
vxdisk -o alldgs list
‚úîÔ∏è Confirm your 1TB shared disk (e.g., /dev/sdb) is in online invalid or error status (not yet initialized)

üìå 2Ô∏è‚É£ Initialize the Disk
On one node:

bash
Copy
Edit
vxdisksetup -i /dev/sdb
‚úîÔ∏è Changes it to auto:none

üìå 3Ô∏è‚É£ Create a Disk Group
On one node (suggested name: sharedg)

bash
Copy
Edit
vxdg -s init sharedg /dev/sdb
‚úîÔ∏è -s makes it shared-capable immediately

üìå 4Ô∏è‚É£ Create a Veritas Volume
On the same node:

bash
Copy
Edit
vxassist -g sharedg make vol1 1000G
‚úîÔ∏è Creates a 1000GB (1TB) volume named vol1

üìå 5Ô∏è‚É£ Create a VxFS Clustered File System
bash
Copy
Edit
mkfs -t vxfs /dev/vx/dsk/sharedg/vol1
‚úîÔ∏è Formats the volume with Veritas File System in cluster mode

üìå 6Ô∏è‚É£ Create a Mount Point Directory
On both nodes:

bash
Copy
Edit
mkdir -p /data1tb
üìå 7Ô∏è‚É£ Mount the File System with Cluster Option
On one node:

bash
Copy
Edit
mount -t vxfs -o cluster /dev/vx/dsk/sharedg/vol1 /data1tb
‚úîÔ∏è -o cluster is mandatory for shared clustered mounts

üìå 8Ô∏è‚É£ Verify Mount
bash
Copy
Edit
df -h /data1tb
And on other node:

bash
Copy
Edit
mount | grep /data1tb
‚úîÔ∏è It should appear and accessible via CFS cluster.

üìå 9Ô∏è‚É£ (Optional) Add to VCS Service Group
If you want to automate mount failover via VCS:

bash
Copy
Edit
hares -add data1tb_mount Mount
hares -modify data1tb_mount MountPoint /data1tb
hares -modify data1tb_mount BlockDevice /dev/vx/dsk/sharedg/vol1
hares -modify data1tb_mount FSType vxfs
hagrp -modify cfs_sg SystemList node01 0 node02 1
hares -group cfs_sg -add data1tb_mount
hagrp -online cfs_sg -sys node01
üìå Recap Table:
Task	Command
Initialize disk	vxdisksetup -i /dev/sdb
Create disk group	vxdg -s init sharedg /dev/sdb
Create volume	vxassist -g sharedg make vol1 1000G
Format filesystem	mkfs -t vxfs /dev/vx/dsk/sharedg/vol1
Create mount point	mkdir -p /data1tb
Mount FS	mount -t vxfs -o cluster /dev/vx/dsk/sharedg/vol1 /data1tb
Verify mount	df -h /data1tb




 vxdisksetup -i /dev/sdb
-bash: vxdisksetup: command not found


üìå Quick Checks & Fix:
‚úÖ 1Ô∏è‚É£ Is VxVM installed?
Check:

bash
Copy
Edit
rpm -qa | grep VRTSvxvm
‚úîÔ∏è You should see something like:

Copy
Edit
VRTSvxvm-7.4.2.3600-RHEL8.x86_64
If not ‚Äî you‚Äôll need to install it via your Veritas package/installer.

‚úÖ 2Ô∏è‚É£ Is the path to vxdisksetup available?
Check if the binary exists:

bash
Copy
Edit
find /opt/VRTS -name vxdisksetup
Typically it should be under:

bash
Copy
Edit
/opt/VRTS/bin/vxdisksetup
‚úÖ 3Ô∏è‚É£ Add Veritas binaries to PATH (if needed)
If the utility exists but isn‚Äôt in your shell‚Äôs PATH:

bash
Copy
Edit
export PATH=$PATH:/opt/VRTS/bin
Then try:

bash
Copy
Edit
vxdisksetup -i /dev/sdb
‚úÖ 4Ô∏è‚É£ If Still Missing ‚Äî Install/Configure VxVM
If neither rpm nor binary is present ‚Äî install VxVM:

bash
Copy
Edit
/opt/VRTS/install/installer
‚Üí Install a Product ‚Üí Veritas InfoScale Storage (or Enterprise)

Once done, verify with:

bash
Copy
Edit
vxdctl -c mode
Should show:

makefile
Copy
Edit
mode: enabled
üìå Summary:
vxdisksetup comes with VRTSvxvm package

Either not installed or not in PATH

Install it or source the binaries



[root@node01 ~]# vxdisksetup -i /dev/sdb
VxVM vxdisksetup ERROR V-5-2-1118 /dev/sdb: Can not get disk information.
[root@node01 ~]#

[root@node01 ~]# vxdg -s init sharedg sdb
VxVM vxdg ERROR V-5-1-19082 Device sdb does not have a label.
Use command 'vxdisksetup' to intialize the disk.

[root@node01 ~]# vxddladm listsupport all | grep sdb
[root@node01 ~]#


[root@node01 ~]# vxdisksetup -i VxDMP5
VxVM  ERROR V-5-1-15404 vxmediadisc: Device path /dev/vx/rdmp/VxDMP5 does not exist.
VxVM  INFO V-5-1-15407 Usage: vxmediadisc [-p] <disk_path>
[root@node01 ~]#


[root@node01 ~]# lsblk | grep 1T
sdb                  8:16   0     1T  0 disk
VxDMP5             201:64   0     1T  0 disk
[root@node01 ~]#

[root@node02 ~]# lsblk | grep 1T
sdb                  8:16   0     1T  0 disk
VxDMP4             201:48   0     1T  0 disk
[root@node02 ~]#



vxdisksetup -i /dev/sdb
VxVM  ERROR V-5-1-15404 vxmediadisc: Device path /dev/vx/rdmp/sdb does not exist.
VxVM  INFO V-5-1-15407 Usage: vxmediadisc [-p] <disk_path>

/dev/vx/dsk/infashared_dg/infashared_vol vxfs       12T   11T  1.7T  86% /infa_shared
vxassist -g sharedg make vol1 1000G


vxassist -g sharedg make share_vol 1000G

mkfs -t vxfs /dev/vx/dsk/sharedg/share_vol


[root@pl-p1-aux-05 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  pl-p1-aux-05         RUNNING              0
A  pl-p1-aux-06         RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             pl-p1-aux-05         Y          N               ONLINE
B  cvm             pl-p1-aux-06         Y          N               ONLINE
B  vip_sg          pl-p1-aux-05         Y          N               OFFLINE
B  vip_sg          pl-p1-aux-06         Y          N               OFFLINE
B  vrts_vea_cfs_int_cfsmount1 pl-p1-aux-05         Y          N               ONLINE
B  vrts_vea_cfs_int_cfsmount1 pl-p1-aux-06         Y          N               ONLINE
B  vxfen           pl-p1-aux-05         Y          N               ONLINE
B  vxfen           pl-p1-aux-06         Y          N               ONLINE
[root@pl-p1-aux-05 ~]#

mkdir -p /infa_shared

mount -t vxfs -o cluster /dev/vx/dsk/sharedg/share_vol /infa_shared



Last login: Sun Jun 22 23:28:48 2025 from 10.253.8.115
[root@node01 ~]# export PATH=$PATH:/opt/VRTS/bin
[root@node01 ~]# vxdisk -e list
DEVICE       TYPE           DISK        GROUP        STATUS               OS_NATIVE_NAME   ATTR
node01_vmdk0_0 auto:none      -            -           online invalid       sdd              -
node01_vmdk0_1 auto:none      -            -           online invalid       sdc              -
node01_vmdk0_2 auto:none      -            -           online invalid       sde              -
node01_vmdk0_3 auto:none      -            -           online invalid       sdb              -
sda          auto:LVM       -            -           LVM                  sda              -
[root@node01 ~]# vxdisksetup -i sdb
VxVM  ERROR V-5-1-15404 vxmediadisc: Device path /dev/vx/rdmp/sdb does not exist.
VxVM  INFO V-5-1-15407 Usage: vxmediadisc [-p] <disk_path>
[root@node01 ~]# vxdisksetup -i node01_vmdk0_3
[root@node01 ~]# vxdisk -e list
DEVICE       TYPE           DISK        GROUP        STATUS               OS_NATIVE_NAME   ATTR
node01_vmdk0_0 auto:none      -            -           online invalid       sdd              -
node01_vmdk0_1 auto:none      -            -           online invalid       sdc              -
node01_vmdk0_2 auto:none      -            -           online invalid       sde              -
node01_vmdk0_3 auto:cdsdisk   -            -           online               sdb              -
sda          auto:LVM       -            -           LVM                  sda              -
[root@node01 ~]# vxdg -s init sharedg node01_vmdk0_3
[root@node01 ~]# vxdisk -e list
DEVICE       TYPE           DISK        GROUP        STATUS               OS_NATIVE_NAME   ATTR
node01_vmdk0_0 auto:none      -            -           online invalid       sdd              -
node01_vmdk0_1 auto:none      -            -           online invalid       sdc              -
node01_vmdk0_2 auto:none      -            -           online invalid       sde              -
node01_vmdk0_3 auto:cdsdisk   node01_vmdk0_3  sharedg     online shared        sdb              -
sda          auto:LVM       -            -           LVM                  sda              -
[root@node01 ~]# vxdisk -e list
DEVICE       TYPE           DISK        GROUP        STATUS               OS_NATIVE_NAME   ATTR
node01_vmdk0_0 auto:none      -            -           online invalid       sdd              -
node01_vmdk0_1 auto:none      -            -           online invalid       sdc              -
node01_vmdk0_2 auto:none      -            -           online invalid       sde              -
node01_vmdk0_3 auto:cdsdisk   node01_vmdk0_3  sharedg     online shared        sdb              -
sda          auto:LVM       -            -           LVM                  sda              -
[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
[root@node01 ~]# vxassist -g sharedg make share_vol 1000G
[root@node01 ~]# vxdisk -e list
DEVICE       TYPE           DISK        GROUP        STATUS               OS_NATIVE_NAME   ATTR
node01_vmdk0_0 auto:none      -            -           online invalid       sdd              -
node01_vmdk0_1 auto:none      -            -           online invalid       sdc              -
node01_vmdk0_2 auto:none      -            -           online invalid       sde              -
node01_vmdk0_3 auto:cdsdisk   node01_vmdk0_3  sharedg     online shared        sdb              -
sda          auto:LVM       -            -           LVM                  sda              -
[root@node01 ~]#

[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
[root@node01 ~]# vxdisk -e list
DEVICE       TYPE           DISK        GROUP        STATUS               OS_NATIVE_NAME   ATTR
node01_vmdk0_0 auto:none      -            -           online invalid       sdd              -
node01_vmdk0_1 auto:none      -            -           online invalid       sdc              -
node01_vmdk0_2 auto:none      -            -           online invalid       sde              -
node01_vmdk0_3 auto:cdsdisk   node01_vmdk0_3  sharedg     online shared        sdb              -
sda          auto:LVM       -            -           LVM                  sda              -
[root@node01 ~]# mkdir -p /infa_shared
[root@node01 ~]# df -Th /infa_shared
Filesystem                 Type  Size  Used Avail Use% Mounted on
/dev/mapper/vg_01-LogVol00 ext4   45G   20G   23G  48% /
[root@node01 ~]# mount -t vxfs -o cluster /dev/vx/dsk/sharedg/share_vol /infa_shared
UX:vxfs mount.vxfs: ERROR: V-3-20012: not a valid vxfs file system
UX:vxfs mount.vxfs: ERROR: V-3-24996: Unable to get disk layout version
[root@node01 ~]# mkfs -t vxfs /dev/vx/dsk/sharedg/share_vol
    version 16 layout
    2097152000 sectors, 1048576000 blocks of size 1024, log size 262144 blocks
    rcq size 16384 blocks
    largefiles supported
    maxlink supported
    WORM not supported
[root@node01 ~]# mount -t vxfs -o cluster /dev/vx/dsk/sharedg/share_vol /infa_shared
[root@node01 ~]# df -Th /infa_shared
Filesystem                    Type  Size  Used Avail Use% Mounted on
/dev/vx/dsk/sharedg/share_vol vxfs 1000G  525M  938G   1% /infa_shared
[root@node01 ~]#

[root@node01 ~]# vxdisk -e list
DEVICE       TYPE           DISK        GROUP        STATUS               OS_NATIVE_NAME   ATTR
node01_vmdk0_0 auto:none      -            -           online invalid       sdd              -
node01_vmdk0_1 auto:none      -            -           online invalid       sdc              -
node01_vmdk0_2 auto:none      -            -           online invalid       sde              -
node01_vmdk0_3 auto:cdsdisk   node01_vmdk0_3  sharedg     online shared        sdb              -
sda          auto:LVM       -            -           LVM                  sda              -
[root@node01 ~]#


[root@node02 ~]# vxdisk -e list
DEVICE       TYPE           DISK        GROUP        STATUS               OS_NATIVE_NAME   ATTR
node01_vmdk0_0 auto:none      -            -           online invalid       sdd              -
node01_vmdk0_1 auto:none      -            -           online invalid       sdc              -
node01_vmdk0_2 auto:none      -            -           online invalid       sde              -
node01_vmdk0_3 auto:cdsdisk   node01_vmdk0_3  sharedg     online shared        sdb              -
sda          auto:LVM       -            -           LVM                  sda              -
[root@node02 ~]# df -Th
Filesystem                 Type      Size  Used Avail Use% Mounted on
devtmpfs                   devtmpfs   16G     0   16G   0% /dev
tmpfs                      tmpfs      16G     0   16G   0% /dev/shm
tmpfs                      tmpfs      16G   34M   16G   1% /run
tmpfs                      tmpfs      16G     0   16G   0% /sys/fs/cgroup
/dev/mapper/vg_01-LogVol00 ext4       45G   14G   29G  33% /
/dev/sda1                  ext4      474M  240M  206M  54% /boot
/dev/mapper/vg_01-LogVol02 ext4       15G  673M   14G   5% /tmp
/dev/mapper/vg_01-LogVol01 ext4      9.6G  212K  9.1G   1% /home
/dev/mapper/vg_01-LogVol03 ext4       20G  1.9G   17G  10% /var
/dev/mapper/vg_01-LogVol04 ext4       15G  1.2G   13G   9% /var/log
/dev/mapper/vg_01-lv_audit ext4      997M   38M  892M   5% /var/log/audit
tmpfs                      tmpfs     3.2G     0  3.2G   0% /run/user/0
[root@node02 ~]# mkdir -p /infa_shared
[root@node02 ~]# mount -t vxfs -o cluster /dev/vx/dsk/sharedg/share_vol /infa_shared
[root@node02 ~]# df -Th
Filesystem                    Type      Size  Used Avail Use% Mounted on
devtmpfs                      devtmpfs   16G     0   16G   0% /dev
tmpfs                         tmpfs      16G     0   16G   0% /dev/shm
tmpfs                         tmpfs      16G   34M   16G   1% /run
tmpfs                         tmpfs      16G     0   16G   0% /sys/fs/cgroup
/dev/mapper/vg_01-LogVol00    ext4       45G   14G   29G  33% /
/dev/sda1                     ext4      474M  240M  206M  54% /boot
/dev/mapper/vg_01-LogVol02    ext4       15G  673M   14G   5% /tmp
/dev/mapper/vg_01-LogVol01    ext4      9.6G  212K  9.1G   1% /home
/dev/mapper/vg_01-LogVol03    ext4       20G  1.9G   17G  10% /var
/dev/mapper/vg_01-LogVol04    ext4       15G  1.2G   13G   9% /var/log
/dev/mapper/vg_01-lv_audit    ext4      997M   38M  892M   5% /var/log/audit
tmpfs                         tmpfs     3.2G     0  3.2G   0% /run/user/0
/dev/vx/dsk/sharedg/share_vol vxfs     1000G  797M  937G   1% /infa_shared
[root@node02 ~]# touch /infa_shared/file123
[root@node02 ~]#


[root@node02 ~]# mkdir -p /infa_shared
[root@node02 ~]# mount -t vxfs -o cluster /dev/vx/dsk/sharedg/share_vol /infa_shared
[root@node02 ~]# df -Th
Filesystem                    Type      Size  Used Avail Use% Mounted on
devtmpfs                      devtmpfs   16G     0   16G   0% /dev
tmpfs                         tmpfs      16G     0   16G   0% /dev/shm
tmpfs                         tmpfs      16G   34M   16G   1% /run
tmpfs                         tmpfs      16G     0   16G   0% /sys/fs/cgroup
/dev/mapper/vg_01-LogVol00    ext4       45G   14G   29G  33% /
/dev/sda1                     ext4      474M  240M  206M  54% /boot
/dev/mapper/vg_01-LogVol02    ext4       15G  673M   14G   5% /tmp
/dev/mapper/vg_01-LogVol01    ext4      9.6G  212K  9.1G   1% /home
/dev/mapper/vg_01-LogVol03    ext4       20G  1.9G   17G  10% /var
/dev/mapper/vg_01-LogVol04    ext4       15G  1.2G   13G   9% /var/log
/dev/mapper/vg_01-lv_audit    ext4      997M   38M  892M   5% /var/log/audit
tmpfs                         tmpfs     3.2G     0  3.2G   0% /run/user/0
/dev/vx/dsk/sharedg/share_vol vxfs     1000G  797M  937G   1% /infa_shared
[root@node02 ~]# touch /infa_shared/file123
[root@node02 ~]#



[root@node01 ~]# ll /infa_shared/file123
-rw-------. 1 root root 0 Jun 22 23:59 /infa_shared/file123
[root@node01 ~]#



[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
[root@node01 ~]# vxdisk -e list
DEVICE       TYPE           DISK        GROUP        STATUS               OS_NATIVE_NAME   ATTR
node01_vmdk0_0 auto:none      -            -           online invalid       sdd              -
node01_vmdk0_1 auto:none      -            -           online invalid       sdc              -
node01_vmdk0_2 auto:none      -            -           online invalid       sde              -
node01_vmdk0_3 auto:cdsdisk   node01_vmdk0_3  sharedg     online shared        sdb              -
sda          auto:LVM       -            -           LVM                  sda              -
[root@node01 ~]#


[root@node02 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
[root@node02 ~]# vxdisk -e list
DEVICE       TYPE           DISK        GROUP        STATUS               OS_NATIVE_NAME   ATTR
node01_vmdk0_0 auto:none      -            -           online invalid       sdd              -
node01_vmdk0_1 auto:none      -            -           online invalid       sdc              -
node01_vmdk0_2 auto:none      -            -           online invalid       sde              -
node01_vmdk0_3 auto:cdsdisk   node01_vmdk0_3  sharedg     online shared        sdb              -
sda          auto:LVM       -            -           LVM                  sda              -
[root@node02 ~]#



[root@pl-p1-aux-05 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  pl-p1-aux-05         RUNNING              0
A  pl-p1-aux-06         RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             pl-p1-aux-05         Y          N               ONLINE
B  cvm             pl-p1-aux-06         Y          N               ONLINE
B  vip_sg          pl-p1-aux-05         Y          N               OFFLINE
B  vip_sg          pl-p1-aux-06         Y          N               OFFLINE
B  vrts_vea_cfs_int_cfsmount1 pl-p1-aux-05         Y          N               ONLINE
B  vrts_vea_cfs_int_cfsmount1 pl-p1-aux-06         Y          N               ONLINE
B  vxfen           pl-p1-aux-05         Y          N               ONLINE
B  vxfen           pl-p1-aux-06         Y          N               ONLINE
[root@pl-p1-aux-05 ~]# vxdisk list
DEVICE          TYPE            DISK         GROUP        STATUS
pl-p1-aux-05_hpe_app0_0 auto:none       -            -            online invalid
pl-p1-aux-05_hpe_app0_1 auto:LVM        -            -            LVM
3pardata0_636 auto:cdsdisk    -            (vxfencoordg) online thinrclm
3pardata0_637 auto:cdsdisk    -            (vxfencoordg) online thinrclm
3pardata0_638 auto:cdsdisk    -            (vxfencoordg) online thinrclm
3pardata0_639 auto:cdsdisk    infashared_dg01  infashared_dg online thinrclm shared
3pardata0_640 auto:cdsdisk    infashared_dg02  infashared_dg online thinrclm shared
3pardata0_641 auto:cdsdisk    infashared_dg03  infashared_dg online thinrclm shared
3pardata0_642 auto:cdsdisk    infashared_dg04  infashared_dg online thinrclm shared
3pardata0_643 auto:cdsdisk    infashared_dg05  infashared_dg online thinrclm shared
3pardata0_644 auto:cdsdisk    infashared_dg06  infashared_dg online thinrclm shared
3pardata0_645 auto:LVM        -            -            LVM
[root@pl-p1-aux-05 ~]# df -Th
Filesystem                               Type      Size  Used Avail Use% Mounted on
devtmpfs                                 devtmpfs  126G     0  126G   0% /dev
tmpfs                                    tmpfs     126G     0  126G   0% /dev/shm
tmpfs                                    tmpfs     126G  4.1G  122G   4% /run
tmpfs                                    tmpfs     126G     0  126G   0% /sys/fs/cgroup
/dev/mapper/vg_01-LogVol00               ext4       34G   22G   11G  68% /
/dev/sda1                                ext4      474M  325M  121M  73% /boot
/dev/mapper/vg_01-LogVol03               ext4       15G  3.2G   11G  24% /var
/dev/mapper/vg_01-LogVol02               ext4       14G  5.2G  7.9G  40% /tmp
/dev/mapper/vg_01-LogVol04               ext4       15G  3.3G   11G  24% /var/log
/dev/mapper/vg_01-LogVol01               ext4       20G   11G  7.9G  58% /home
/dev/mapper/vg_01-lv_audit               ext4      997M   35M  895M   4% /var/log/audit
/dev/mapper/vg_02-lv_orasw               xfs       4.0G  1.8G  2.3G  45% /orasw
/dev/mapper/vg_02-lv_app                 xfs        70G   58G   13G  83% /app
/dev/mapper/vg_02-lv_cache               xfs       590G   15G  576G   3% /cache
/dev/vx/dsk/infashared_dg/infashared_vol vxfs       12T   11T  1.7T  86% /infa_shared
tmpfs                                    tmpfs      26G     0   26G   0% /run/user/7003
tmpfs                                    tmpfs      26G     0   26G   0% /run/user/4112
tmpfs                                    tmpfs      26G     0   26G   0% /run/user/4113
tmpfs                                    tmpfs      26G     0   26G   0% /run/user/513202344
tmpfs                                    tmpfs      26G     0   26G   0% /run/user/513202535
[root@pl-p1-aux-05 ~]#



cfsmntadm add -m /infa_shared -t vxfs -d /dev/vx/dsk/sharedg/share_vol



üìå Summary:
Action	Command
Probe resource manually	hares -probe cfsmount1 -sys node-01
Check resource status	hastatus -sum
Check cluster services	lltstat -nvv, gabconfig -a
Verify volume + mount path	ls -l /dev/vx/dsk/sharedg/share_vol
Bring group online if ready	hagrp -online cfs-sg -sys node-01


KSP Ref:

hares -add data1tb_mount Mount
hares -modify data1tb_mount MountPoint /data1tb
hares -modify data1tb_mount BlockDevice /dev/vx/dsk/sharedg/vol1
hares -modify data1tb_mount FSType vxfs
hagrp -modify cfs_sg SystemList node01 0 node02 1
hares -group cfs_sg -add data1tb_mount
hagrp -online cfs_sg -sys node01

for /dev/vx/dsk/sharedg/share_vol


Dinesh

hagrp -add cfs-sg node01 node02
hares -add cfsvol1 Volume cfs-sg
hares -modify cfsvol1 diskgroup shared
hares -modify cfsvol1 volume share_vol
hares -modify cfsvol1 enable 1
 
hares -add cfsmount1 mount cfs-sg
hares -modify cfsmount1 BlockDevice /dev/vx/dsk/sharedg/share_vol 
hares -modify cfsmount1 MountOpt rw,cluster,crw,delaylog,largefiles,ioerror=mdisable,noatime,nomtime
hares -modify cfsmount1 MountPoint /infa_shared
hares -modify cfsmount1 FSType vxfs
hares -modify cfsmount1 Enabled 1

  762  23/06/25 06:26:39 haconf -makerw
  763  23/06/25 06:26:50 hagrp -create cfs-sg node01 node02
  764  23/06/25 06:27:42 hagrp -add cfs-sg node01 node02
  765  23/06/25 06:29:00 hagrp -add cfs-sg
  766  23/06/25 06:29:56 hastatus -sum
  767  23/06/25 06:31:14 hagrp -add cfs-sg
  768  23/06/25 06:32:22 hagrp -modify cfs-sg SystemList node01 node02
  769  23/06/25 06:32:43 hagrp -modify cfs-sg SystemList node01 1 node02 2
  770  23/06/25 06:33:00 hares -add cfsvol1 volume cfs-sg
  771  23/06/25 06:33:21 hares -add cfsvol1 Volume cfs-sg
  772  23/06/25 06:33:37 hares -modify cfsvol1 diskgroup shared
  773  23/06/25 06:34:55 hares -modify cfsvol1 DiskGroup shared
  774  23/06/25 06:35:29 hares -modify cfsvol1 Volume share_vol
  775  23/06/25 06:35:38 hares -modify cfsvol1 enabled 1
  776  23/06/25 06:35:55 hares -modify cfsvol1 enable 1
  777  23/06/25 06:36:45 hares -modify cfsvol1 Enabled 1
  778  23/06/25 06:37:01 hares -add cfsmount1 mount cfs-sg
  779  23/06/25 06:37:09 hares -add cfsmount1 Mount cfs-sg
  780  23/06/25 06:37:19 hares -modify cfsmount1 BlockDevice /dev/vx/dsk/sharedg/share_vol
  781  23/06/25 06:37:29 hares -modify cfsmount1 MountOpt rw,cluster,crw,delaylog,largefiles,ioerror=mdisable,noatime,nomtime
  782  23/06/25 06:37:38 hares -modify cfsmount1 MountPoint /infa_shared
  783  23/06/25 06:37:46 hares -modify cfsmount1 FSType vxfs
  784  23/06/25 06:37:58 hares -modify cfsmount1 Enabled 1
  785  23/06/25 06:38:12 haconf -dump -makero
  786  23/06/25 06:38:19 hastatus -sum
  787  23/06/25 06:39:56 df -h
  788  23/06/25 06:40:01 hastatus -sum
  789  23/06/25 06:40:17 hagrp -online node01
  790  23/06/25 06:41:07 hagrp -online cfs-sg -sys node01
  791  23/06/25 06:41:16 hastatus -sum
  792  23/06/25 06:43:19 cat /etc/VRTSvcs/conf/config/types.cf
  793  23/06/25 06:43:40 cat /etc/VRTSvcs/conf/types.cf
  794  23/06/25 06:45:44 hares -modify cfsmount1 Type CFSMount
  795  23/06/25 06:45:54 haconf -makerw
  796  23/06/25 06:46:00 hares -modify cfsmount1 Type CFSMount
  797  23/06/25 06:46:15 hares -modify cfsmount1 type CFSMount
  798  23/06/25 06:47:24 hares -modify cfsmount1 AMFMountType vxfs
  799  23/06/25 06:47:34 hastatus -sum
  800  23/06/25 06:47:57 tail -f /var/VRTSvcs/log/engine_A.log
  801  23/06/25 06:50:34 hares -probe cfsmount1 -sys node01
  802  23/06/25 06:50:40 hastatus -sum
  803  23/06/25 06:50:46 tail -f /var/VRTSvcs/log/engine_A.log
  804  23/06/25 06:51:33 hacf -verify /etc/VRTSvcs/conf/config
  805  23/06/25 06:52:19 ll /etc/VRTSvcs/conf/config/types.cf
  806  23/06/25 06:53:26 ll /etc/VRTSvcs/conf/types.cf
  807  23/06/25 06:53:37 more /etc/VRTSvcs/conf/types.cf
  808  23/06/25 06:55:20
  809  23/06/25 06:56:11 hastop -all -force
  810  23/06/25 06:56:35 hastatus -sum
  811  23/06/25 06:56:39 mv /etc/VRTSvcs/conf/config/types.cf /etc/VRTSvcs/conf/config/types.cf.bak
  812  23/06/25 06:56:55 ls -lrth /etc/VRTSvcs/conf/config/types.cf*
  813  23/06/25 06:57:26 cp /etc/VRTSvcs/conf/types.cf /etc/VRTSvcs/conf/config/types.cf
  814  23/06/25 06:57:44 ls -l /etc/VRTSvcs/conf/types.cf
  815  23/06/25 06:57:46 ls -l /etc/VRTSvcs/conf/config/types.cf
  816  23/06/25 06:57:55 hastart
  817  23/06/25 06:58:48 hastatus -sum
  818  23/06/25 07:00:11 hastop -all -force
  819  23/06/25 07:00:51 hacf -verify /etc/VRTSvcs/conf/config
  820  23/06/25 07:01:11 hastart
  821  23/06/25 07:01:36 hastatus -sum
  822  23/06/25 07:12:11 aconf -makerw
  823  23/06/25 07:12:16 haconf -makerw
  824  23/06/25 07:12:27 hares -modify cfsmount1 FsckOpt "n"
  825  23/06/25 07:13:11 hagrp -online cfs-sg -sys node01 node02
  826  23/06/25 07:13:25 hastatus -sum
  827  23/06/25 07:13:49 haconf -dump -makero
  828  23/06/25 07:17:18 hastatus -sum
  829  23/06/25 07:17:31 hastart -all
  830  23/06/25 07:17:40 hastart
  831  23/06/25 07:24:55 hastatus -sum
  832  23/06/25 07:25:04 hastart
  833  23/06/25 07:30:48 exit
  834  23/06/25 19:14:03 hastatus -sum
  835  23/06/25 19:14:18 lslbk
  836  23/06/25 19:14:21 lsblk
  837  23/06/25 19:14:53 hastatus -sum
  838  23/06/25 19:15:13 vxdisk -o alldgs -e list
  839  23/06/25 19:15:42 vxdg list
  840  23/06/25 19:15:57 vxprint -Pl
  841  23/06/25 19:16:09 vxprint -htq
  842  23/06/25 22:19:14 vxdisk -o alldgs -e list
  843  23/06/25 22:19:19 hastatus -sum
  844  23/06/25 22:20:53 lsblk
  845  23/06/25 22:22:23 hastatus -sum
  846  23/06/25 22:23:23 haconf -makerw
  847  23/06/25 22:23:46 hares -probe cfsmount1 -sys node01
  848  23/06/25 22:23:53 hares -probe cfsmount1 -sys node02
  849  23/06/25 22:24:02 haconf -dump -makero
  850  23/06/25 22:24:07 hastatus -sum
  851  23/06/25 22:24:33 hagrp -online cfs-sg -sys node01
  852  23/06/25 22:25:19 hastatus -sum
  853  23/06/25 22:25:26 ps -ef | grep -i had
  854  23/06/25 22:26:10 haconf -makerw
  855  23/06/25 22:26:10 hagrp -enable cfs-sg
  856  23/06/25 22:26:10 haconf -dump -makero
  857  23/06/25 22:26:17 hastatus -sum
  858  23/06/25 22:26:27 hagrp -online cfs-sg -sys node01
  859  23/06/25 22:27:09 df -Th
  860  23/06/25 22:28:55 lltstat -nvv
  861  23/06/25 22:29:15 gabconfig -a
  862  23/06/25 22:30:43 cat /etc/llttab
  863  23/06/25 22:33:00 hastatus -sum
  864  23/06/25 22:33:43 vxprint -htq
  865  23/06/25 22:34:30 history





hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cfs-sg          node01     N          Y               OFFLINE
B  cfs-sg          node02     N          Y               OFFLINE
B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  vrts_vea_cfs_int_cfsmount2 node01     Y          N               ONLINE
B  vrts_vea_cfs_int_cfsmount2 node02     Y          N               ONLINE

-- RESOURCES NOT PROBED
-- Group           Type                 Resource             System

E  cfs-sg          Mount                cfsmount1            node01
E  cfs-sg          Mount                cfsmount1            node02


group cfs-sg (
        SystemList = { node01 = 1, node02 = 2 }
        Enabled @node01 = 0
        Enabled @node02 = 0
        )

        Volume cfsvol1 (
                DiskGroup = shared
                Volume = share_vol
                )



        // resource dependency tree
        //
        //      group cfs-sg
        //      {
        //      Volume cfsvol1
        //      }


VCS WARNING V-16-1-11335 Configuration must be ReadWrite : Use haconf -makerw
hagrp -delete vrts_vea_cfs_int_cfsmount2
hagrp -delete cfs-sg
hagrp -dep
hares -list | grep cfs-sg
haconf -dump -makero # Save and lock:
hacf -verify /etc/VRTSvcs/conf/config

hastop -all
hastart

hastop -all
VCS WARNING V-16-1-10028 Cluster configuration is writable.  Must do 'haconf -dump -makero' or 'hastop -all -force'

ll /dev/vx/dsk/sharedg/share_vol /infa_shared



[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
[root@node01 ~]# ll /dev/vx/dsk/sharedg/share_vol /infa_shared
brw-------. 1 root root 199, 37000 Jun 24 04:01 /dev/vx/dsk/sharedg/share_vol



 hares -display infa_mount | grep -E "infa_shared|vxfs|sharedg"
infa_mount   ArgListValues         node01 MountPoint  1       /infa_shared    BlockDevice     1       /dev/vx/dsk/sharedg/share_vol FSType  1       vxfs    MountOpt        1       rw,cluster,crw,delaylog,largefiles,ioerror=mdisable,noatime,nomtime     FsckOpt       1       ""      SnapUmount      1       0       CkptUmount      1       1       BindUmount      1       1       OptCheck     10       CreateMntPt     1       0       MntPtPermission 1       ""      MntPtOwner      1       ""      MntPtGroup      1       ""   AccessPermissionChk      1       0       RecursiveMnt    1       0       VxFSMountLock   1       1       CacheRestoreAccess      1    0
infa_mount   ArgListValues         node02 MountPoint  1       /infa_shared    BlockDevice     1       /dev/vx/dsk/sharedg/share_vol FSType  1       vxfs    MountOpt        1       rw,cluster,crw,delaylog,largefiles,ioerror=mdisable,noatime,nomtime     FsckOpt       1       ""      SnapUmount      1       0       CkptUmount      1       1       BindUmount      1       1       OptCheck     10       CreateMntPt     1       0       MntPtPermission 1       ""      MntPtOwner      1       ""      MntPtGroup      1       ""   AccessPermissionChk      1       0       RecursiveMnt    1       0       VxFSMountLock   1       1       CacheRestoreAccess      1    0
infa_mount   BlockDevice           global           /dev/vx/dsk/sharedg/share_vol
infa_mount   FSType                global           vxfs
infa_mount   MountPoint            global           /infa_shared
[root@node01 ~]#


/infa_shared:
total 0
[root@node01 ~]#



Perfect ‚Äî let‚Äôs cleanly set up a new service group and mount resource to manage your /infa_shared Veritas clustered file system via VCS.
I‚Äôll give you step-by-step safe, production-grade commands for it.

üìå üìñ Prerequisites:
/dev/vx/dsk/sharedg/share_vol exists ‚úÖ

/infa_shared mount point exists ‚úÖ

cvm group is online ‚úÖ

Cluster status healthy ‚úÖ

üìå üìä New Service Group & Resource Setup
1Ô∏è‚É£ Create Service Group
bash
Copy
Edit
hagrp -add infa_sg
Add both nodes to the SystemList:

bash
Copy
Edit
hagrp -modify infa_sg SystemList node01 0 node02 1
2Ô∏è‚É£ Create Mount Resource
bash
Copy
Edit
hares -add infa_mount Mount
3Ô∏è‚É£ Configure Mount Resource Attributes
bash
Copy
Edit
hares -modify infa_mount MountPoint /infa_shared
hares -modify infa_mount BlockDevice /dev/vx/dsk/sharedg/share_vol
hares -modify infa_mount FSType vxfs
hares -modify infa_mount FsckOpt "-n"
hares -modify infa_mount MountOpt "cluster"
4Ô∏è‚É£ Link Resource to Service Group
bash
Copy
Edit
hares -group infa_sg -add infa_mount
5Ô∏è‚É£ Bring Service Group Online on one node
bash
Copy
Edit
hagrp -online infa_sg -sys node01
6Ô∏è‚É£ Check Status
bash
Copy
Edit
hastatus -sum
You should now see infa_sg online on node01 and infa_mount probed and mounted.

7Ô∏è‚É£ Verify Mount
bash
Copy
Edit
df -h /infa_shared
Confirm it‚Äôs mounted via Veritas CFS.

üìå üìí Optional: AutoStart Configuration (if desired)
If you want this service group to auto start:

bash
Copy
Edit
hagrp -modify infa_sg AutoStartList node01 node02
‚úÖ Final Check:
hastatus -sum ‚Äî verify ONLINE

df -h ‚Äî mount point accessible

mount ‚Äî should show mounted with vxfs and cluster option





[root@node01 ~]# hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     N          N               OFFLINE
B  infa_sg         node02     N          N               OFFLINE

-- RESOURCES NOT PROBED
-- Group           Type                 Resource             System

E  infa_sg         Mount                infa_mount           node01
E  infa_sg         Mount                infa_mount           node02



 hastatus -sum

-- SYSTEM STATE
-- System               State                Frozen

A  node01     RUNNING              0
A  node02     RUNNING              0

-- GROUP STATE
-- Group           System               Probed     AutoDisabled    State

B  cvm             node01     Y          N               ONLINE
B  cvm             node02     Y          N               ONLINE
B  infa_sg         node01     N          Y               OFFLINE
B  infa_sg         node02     N          Y               OFFLINE

-- RESOURCES NOT PROBED
-- Group           Type                 Resource             System

E  infa_sg         Mount                infa_mount           node01
E  infa_sg         Mount                infa_mount           node02



cat /etc/VRTSvcs/conf/config/main.cf
include "OracleASMTypes.cf"
include "types.cf"
include "CFSTypes.cf"
include "CRSResource.cf"
include "CSSD.cf"
include "CVMTypes.cf"
include "Db2udbTypes.cf"
include "MultiPrivNIC.cf"
include "OracleTypes.cf"
include "PrivNIC.cf"
include "SybaseTypes.cf"

cluster auxlab_poc (
        ProtocolNumber = 10000
        SecureClus = 1
        GuestGroups = { testgrp }
        UseFence = SCSI3
        HacliUserLevel = COMMANDROOT
        )

system node01 (
        )

system node02 (
        )

group cvm (
        SystemList = { node01 = 0, node02 = 1 }
        AutoFailOver = 0
        Parallel = 1
        AutoStartList = { node01, node02 }
        )

        CFSfsckd vxfsckd (
                ActivationMode @node01 = { sharedg = sw }
                ActivationMode @node02 = { sharedg = sw }
                )

        CVMCluster cvm_clus (
                CVMClustName = auxlab_poc
                CVMNodeId = { node01 = 0, node02 = 1 }
                CVMTransport = gab
                CVMTimeout = 200
                )

        CVMVxconfigd cvm_vxconfigd (
                Critical = 0
                CVMVxconfigdArgs = { syslog }
                )

        ProcessOnOnly vxattachd (
                Critical = 0
                PathName = "/bin/sh"
                Arguments = "- /usr/lib/vxvm/bin/vxattachd root"
                RestartLimit = 3
                )

        cvm_clus requires cvm_vxconfigd
        vxfsckd requires cvm_clus


        // resource dependency tree
        //
        //      group cvm
        //      {
        //      ProcessOnOnly vxattachd
        //      CFSfsckd vxfsckd
        //          {
        //          CVMCluster cvm_clus
        //              {
        //              CVMVxconfigd cvm_vxconfigd
        //              }
        //          }
        //      }


group infa_sg (
        SystemList = { node01 = 0, node02 = 1 }
        )

        Mount infa_mount (
                MountPoint = "/infa_shared"
                BlockDevice = "/dev/vx/dsk/sharedg/share_vol"
                FSType = vxfs
                MountOpt = "cluster"
                )



        // resource dependency tree
        //
        //      group infa_sg
        //      {
        //      Mount infa_mount
        //      }









#!/bin/bash

# -----------------------------------------------------------------------------
# Veritas Cluster Server (VCS) Service Group and Resource Setup for CVM+CFS
# -----------------------------------------------------------------------------
# Author: You üòâ
# Usage: Run this on any cluster node where VCS CLI is configured
# -----------------------------------------------------------------------------

# 1Ô∏è‚É£ Check GAB membership (ensure CVM is running and cluster is healthy)
echo "Checking GAB port membership..."
gabconfig -a

# 2Ô∏è‚É£ Verify CVM service group state (should be ONLINE on both nodes)
echo "Checking existing service group states..."
hagrp -state

# 3Ô∏è‚É£ Create new service group: infa_sg
echo "Creating service group 'infa_sg'..."
hagrp -add infa_sg

# Set system list for the group
hagrp -modify infa_sg SystemList node01 0 node02 1

# Define autostart list
hagrp -modify infa_sg AutoStartList node01 node02

# Set group to be Parallel (for CFS mount groups)
hagrp -modify infa_sg AutoFailOver 0
hagrp -modify infa_sg Parallel 1

# 4Ô∏è‚É£ Add CFSMount resource: infa_mount
echo "Adding CFSMount resource 'infa_mount'..."
hares -add infa_mount CFSMount infa_sg

# Configure CFSMount resource attributes
hares -modify infa_mount MountPoint "/infa_shared"
hares -modify infa_mount BlockDevice "/dev/vx/dsk/sharedg/share_vol"
hares -modify infa_mount FSType vxfs
hares -modify infa_mount MountOpt "cluster"

# 5Ô∏è‚É£ Enable service group on both nodes
echo "Enabling service group on both nodes..."
hagrp -enable infa_sg -sys node01
hagrp -enable infa_sg -sys node02

# 6Ô∏è‚É£ Optional: Create dependency if needed (between cvm and infa_sg if both are Parallel)
echo "Creating group dependency (if applicable)..."
hagrp -link cvm infa_sg online local firm

# 7Ô∏è‚É£ Verify configuration integrity
echo "Verifying configuration..."
hacf -verify /etc/VRTSvcs/conf/config

# 8Ô∏è‚É£ Commit the config if no errors
echo "Committing configuration..."
hacf -commit /etc/VRTSvcs/conf/config

# 9Ô∏è‚É£ Bring infa_sg service group online on node01
echo "Bringing service group online on node01..."
hagrp -online infa_sg -sys node01

#  üîü Show final state summary
echo "Cluster status summary:"
hastatus -sum

echo "‚úÖ Configuration completed successfully."




[root@node01 ~]# cat vcs_health_check.sh
#!/bin/bash

# -----------------------------------------------------------------------------
# Veritas Cluster Server (VCS) Health Check Script
# -----------------------------------------------------------------------------
# Author: You üòâ
# Purpose: Check overall cluster, GAB, LLT, service groups, and resource states
# Usage: Run as root or VCS-enabled user on any cluster node
# -----------------------------------------------------------------------------

#CLUSTER_NAME=$(haconf -display | grep "ClusterName" | awk '{print $3}')
#echo "üì° Cluster Name: $CLUSTER_NAME"
#echo "===================="

# Fetch cluster name from main.cf
CLUSTER_NAME=$(grep -i '^cluster' /etc/VRTSvcs/conf/config/main.cf | awk '{print $2}' | tr -d '()')

echo "üì° Cluster Name: $CLUSTER_NAME"
echo "===================="


# 1Ô∏è‚É£ Check LLT links and port status
echo "üîç LLT Link Status:"
#lltstat -nvv | grep -E "Node|Link|Status"
lltstat -nvv | head -10
echo "--------------------"

# 2Ô∏è‚É£ Check GAB port membership
echo "üîç GAB Port Memberships:"
gabconfig -a

echo "--------------------"

# 3Ô∏è‚É£ Check Cluster Systems' Status
echo "üîç Cluster Nodes Status:"
hastatus -sum | grep "SYSTEM STATE" -A 5

echo "--------------------"

# 4Ô∏è‚É£ Check Service Groups State Summary
echo "üîç Service Groups Summary:"
hastatus -sum | grep "GROUP STATE" -A 10

echo "--------------------"

# 5Ô∏è‚É£ Check for any failed resources
echo "üîç Failed Resources (if any):"
hastatus -sum | grep "RESOURCES FAILED" -A 5

echo "--------------------"

# 6Ô∏è‚É£ Check specific service group status (example: cvm, infa_sg)
for grp in cvm infa_sg; do
  echo "üîç Service Group: $grp"
  hagrp -state $grp
  echo "--------------------"
done

# 7Ô∏è‚É£ Check if any groups are frozen
echo "üîç Frozen Groups:"
hagrp -list | grep -i Frozen

echo "--------------------"

# 8Ô∏è‚É£ Check for offline/faulted resources
echo "üîç Resources Offline/Faulted:"
hares -state | egrep "OFFLINE|FAULTED"

echo "===================="
echo "‚úÖ Cluster health check completed."

[root@node01 ~]#


